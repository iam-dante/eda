{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_file = \"./doc/1706.03762.pdf\"\n",
    "pdf_file = \"./doc/2005.11401.pdf\"\n",
    "text_file = \"./doc/textfile.txt\"\n",
    "brian_pdf = \"./doc/Brian's_Resume.pdf\"\n",
    "opt_workshop = \"./doc/opt_workshop_umbc.pdf\"\n",
    "eda_chatper = \"./doc/eda-chapter.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 4\\nExploratory Data Analysis\\nA ﬁrst look at the data.\\nAs mentioned in Chapter 1, exploratory data analysis or “EDA” is a critical\\nﬁrst step in analyzing the data from an experiment. Here are the main reasons we\\nuse EDA:\\n• detection of mistakes\\n• checking of assumptions\\n• preliminary selection of appropriate models\\n• determining relationships among the explanatory variables, and\\n• assessing the direction and rough size of relationships between explanatory\\nand outcome variables.\\nLoosely speaking, any method of looking at data that does not include formal\\nstatistical modeling and inference falls under the term exploratory data analysis.\\n4.1\\nTypical data format and the types of EDA\\nThe data from an experiment are generally collected into a rectangular array (e.g.,\\nspreadsheet or database), most commonly with one row per experimental subject\\n61\\n\\n62\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nand one column for each subject identiﬁer, outcome variable, and explanatory\\nvariable. Each column contains the numeric values for a particular quantitative\\nvariable or the levels for a categorical variable. (Some more complicated experi-\\nments require a more complex data layout.)\\nPeople are not very good at looking at a column of numbers or a whole spread-\\nsheet and then determining important characteristics of the data. They ﬁnd look-\\ning at numbers to be tedious, boring, and/or overwhelming. Exploratory data\\nanalysis techniques have been devised as an aid in this situation. Most of these\\ntechniques work in part by hiding certain aspects of the data while making other\\naspects more clear.\\nExploratory data analysis is generally cross-classiﬁed in two ways. First, each\\nmethod is either non-graphical or graphical. And second, each method is either\\nunivariate or multivariate (usually just bivariate).\\nNon-graphical methods generally involve calculation of summary statistics,\\nwhile graphical methods obviously summarize the data in a diagrammatic or pic-\\ntorial way.\\nUnivariate methods look at one variable (data column) at a time,\\nwhile multivariate methods look at two or more variables at a time to explore\\nrelationships. Usually our multivariate EDA will be bivariate (looking at exactly\\ntwo variables), but occasionally it will involve three or more variables. It is almost\\nalways a good idea to perform univariate EDA on each of the components of a\\nmultivariate EDA before performing the multivariate EDA.\\nBeyond the four categories created by the above cross-classiﬁcation, each of the\\ncategories of EDA have further divisions based on the role (outcome or explana-\\ntory) and type (categorical or quantitative) of the variable(s) being examined.\\nAlthough there are guidelines about which EDA techniques are useful in what\\ncircumstances, there is an important degree of looseness and art to EDA. Com-\\npetence and conﬁdence come with practice, experience, and close observation of\\nothers.\\nAlso, EDA need not be restricted to techniques you have seen before;\\nsometimes you need to invent a new way of looking at your data.\\nThe four types of EDA are univariate non-graphical, multivariate non-\\ngraphical, univariate graphical, and multivariate graphical.\\nThis chapter ﬁrst discusses the non-graphical and graphical methods for looking\\n\\n4.2. UNIVARIATE NON-GRAPHICAL EDA\\n63\\nat single variables, then moves on to looking at multiple variables at once, mostly\\nto investigate the relationships between the variables.\\n4.2\\nUnivariate non-graphical EDA\\nThe data that come from making a particular measurement on all of the subjects in\\na sample represent our observations for a single characteristic such as age, gender,\\nspeed at a task, or response to a stimulus. We should think of these measurements\\nas representing a “sample distribution” of the variable, which in turn more or\\nless represents the “population distribution” of the variable. The usual goal of\\nunivariate non-graphical EDA is to better appreciate the “sample distribution”\\nand also to make some tentative conclusions about what population distribution(s)\\nis/are compatible with the sample distribution. Outlier detection is also a part of\\nthis analysis.\\n4.2.1\\nCategorical data\\nThe characteristics of interest for a categorical variable are simply the range of\\nvalues and the frequency (or relative frequency) of occurrence for each value. (For\\nordinal variables it is sometimes appropriate to treat them as quantitative vari-\\nables using the techniques in the second part of this section.) Therefore the only\\nuseful univariate non-graphical techniques for categorical variables is some form of\\ntabulation of the frequencies, usually along with calculation of the fraction (or\\npercent) of data that falls in each category. For example if we categorize subjects\\nby College at Carnegie Mellon University as H&SS, MCS, SCS and “other”, then\\nthere is a true population of all students enrolled in the 2007 Fall semester. If we\\ntake a random sample of 20 students for the purposes of performing a memory ex-\\nperiment, we could list the sample “measurements” as H&SS, H&SS, MCS, other,\\nother, SCS, MCS, other, H&SS, MCS, SCS, SCS, other, MCS, MCS, H&SS, MCS,\\nother, H&SS, SCS. Our EDA would look like this:\\nStatistic/College\\nH&SS\\nMCS\\nSCS\\nother\\nTotal\\nCount\\n5\\n6\\n4\\n5\\n20\\nProportion\\n0.25\\n0.30\\n0.20\\n0.25\\n1.00\\nPercent\\n25%\\n30%\\n20%\\n25%\\n100%\\nNote that it is useful to have the total count (frequency) to verify that we\\n\\n64\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nhave an observation for each subject that we recruited. (Losing data is a common\\nmistake, and EDA is very helpful for ﬁnding mistakes.). Also, we should expect\\nthat the proportions add up to 1.00 (or 100%) if we are calculating them correctly\\n(count/total). Once you get used to it, you won’t need both proportion (relative\\nfrequency) and percent, because they will be interchangeable in your mind.\\nA simple tabulation of the frequency of each category is the best\\nunivariate non-graphical EDA for categorical data.\\n4.2.2\\nCharacteristics of quantitative data\\nUnivariate EDA for a quantitative variable is a way to make prelim-\\ninary assessments about the population distribution of the variable\\nusing the data of the observed sample.\\nThe characteristics of the population distribution of a quantitative variable are\\nits center, spread, modality (number of peaks in the pdf), shape (including “heav-\\niness of the tails”), and outliers. (See section 3.5.) Our observed data represent\\njust one sample out of an inﬁnite number of possible samples. The characteristics\\nof our randomly observed sample are not inherently interesting, except to the degree\\nthat they represent the population that it came from.\\nWhat we observe in the sample of measurements for a particular variable that\\nwe select for our particular experiment is the “sample distribution”.\\nWe need\\nto recognize that this would be diﬀerent each time we might repeat the same\\nexperiment, due to selection of a diﬀerent random sample, a diﬀerent treatment\\nrandomization, and diﬀerent random (incompletely controlled) experimental con-\\nditions. In addition we can calculate “sample statistics” from the data, such as\\nsample mean, sample variance, sample standard deviation, sample skewness and\\nsample kurtosis. These again would vary for each repetition of the experiment, so\\nthey don’t represent any deep truth, but rather represent some uncertain informa-\\ntion about the underlying population distribution and its parameters, which are\\nwhat we really care about.\\n\\n4.2. UNIVARIATE NON-GRAPHICAL EDA\\n65\\nMany of the sample’s distributional characteristics are seen qualitatively in the\\nunivariate graphical EDA technique of a histogram (see 4.3.1). In most situations it\\nis worthwhile to think of univariate non-graphical EDA as telling you about aspects\\nof the histogram of the distribution of the variable of interest. Again, these aspects\\nare quantitative, but because they refer to just one of many possible samples from\\na population, they are best thought of as random (non-ﬁxed) estimates of the\\nﬁxed, unknown parameters (see section 3.5) of the distribution of the population\\nof interest.\\nIf the quantitative variable does not have too many distinct values, a tabula-\\ntion, as we used for categorical data, will be a worthwhile univariate, non-graphical\\ntechnique.\\nBut mostly, for quantitative variables we are concerned here with\\nthe quantitative numeric (non-graphical) measures which are the various sam-\\nple statistics. In fact, sample statistics are generally thought of as estimates of\\nthe corresponding population parameters.\\nFigure 4.1 shows a histogram of a sample of size 200 from the inﬁnite popula-\\ntion characterized by distribution C of ﬁgure 3.1 from section 3.5. Remember that\\nin that section we examined the parameters that characterize theoretical (pop-\\nulation) distributions. Now we are interested in learning what we can (but not\\neverything, because parameters are “secrets of nature”) about these parameters\\nfrom measurements on a (random) sample of subjects out of that population.\\nThe bi-modality is visible, as is an outlier at X=-2. There is no generally\\nrecognized formal deﬁnition for outlier, but roughly it means values that are outside\\nof the areas of a distribution that would commonly occur. This can also be thought\\nof as sample data values which correspond to areas of the population pdf (or pmf)\\nwith low density (or probability). The deﬁnition of “outlier” for standard boxplots\\nis described below (see 4.3.3). Another common deﬁnition of “outlier” consider\\nany point more than a ﬁxed number of standard deviations from the mean to be\\nan “outlier”, but these and other deﬁnitions are arbitrary and vary from situation\\nto situation.\\nFor quantitative variables (and possibly for ordinal variables) it is worthwhile\\nlooking at the central tendency, spread, skewness, and kurtosis of the data for a\\nparticular variable from an experiment. But for categorical variables, none of these\\nmake any sense.\\n\\n66\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nX\\nFrequency\\n−2\\n−1\\n0\\n1\\n2\\n3\\n4\\n5\\n0\\n5\\n10\\n15\\n20\\nFigure 4.1: Histogram from distribution C.\\n\\n4.2. UNIVARIATE NON-GRAPHICAL EDA\\n67\\n4.2.3\\nCentral tendency\\nThe central tendency or “location” of a distribution has to do with typical or\\nmiddle values. The common, useful measures of central tendency are the statis-\\ntics called (arithmetic) mean, median, and sometimes mode. Occasionally other\\nmeans such as geometric, harmonic, truncated, or Winsorized means are used as\\nmeasures of centrality. While most authors use the term “average” as a synonym\\nfor arithmetic mean, some use average in a broader sense to also include geometric,\\nharmonic, and other means.\\nAssuming that we have n data values labeled x1 through xn, the formula for\\ncalculating the sample (arithmetic) mean is\\n¯x =\\nPn\\ni=1 xi\\nn\\n.\\nThe arithmetic mean is simply the sum of all of the data values divided by the\\nnumber of values. It can be thought of as how much each subject gets in a “fair”\\nre-division of whatever the data are measuring. For instance, the mean amount\\nof money that a group of people have is the amount each would get if all of the\\nmoney were put in one “pot”, and then the money was redistributed to all people\\nevenly. I hope you can see that this is the same as “summing then dividing by n”.\\nFor any symmetrically shaped distribution (i.e., one with a symmetric his-\\ntogram or pdf or pmf) the mean is the point around which the symmetry holds.\\nFor non-symmetric distributions, the mean is the “balance point”: if the histogram\\nis cut out of some homogeneous stiﬀmaterial such as cardboard, it will balance on\\na fulcrum placed at the mean.\\nFor many descriptive quantities, there are both a sample and a population ver-\\nsion. For a ﬁxed ﬁnite population or for a theoretic inﬁnite population described\\nby a pmf or pdf, there is a single population mean which is a ﬁxed, often unknown,\\nvalue called the mean parameter (see section 3.5). On the other hand, the “sam-\\nple mean” will vary from sample to sample as diﬀerent samples are taken, and so is\\na random variable. The probability distribution of the sample mean is referred to\\nas its sampling distribution. This term expresses the idea that any experiment\\ncould (at least theoretically, given enough resources) be repeated many times and\\nvarious statistics such as the sample mean can be calculated each time. Often\\nwe can use probability theory to work out the exact distribution of the sample\\nstatistic, at least under certain assumptions.\\nThe median is another measure of central tendency. The sample median is\\n\\n68\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nthe middle value after all of the values are put in an ordered list. If there are an\\neven number of values, take the average of the two middle values. (If there are ties\\nat the middle, some special adjustments are made by the statistical software we\\nwill use. In unusual situations for discrete random variables, there may not be a\\nunique median.)\\nFor symmetric distributions, the mean and the median coincide. For unimodal\\nskewed (asymmetric) distributions, the mean is farther in the direction of the\\n“pulled out tail” of the distribution than the median is.\\nTherefore, for many\\ncases of skewed distributions, the median is preferred as a measure of central\\ntendency. For example, according to the US Census Bureau 2004 Economic Survey,\\nthe median income of US families, which represents the income above and below\\nwhich half of families fall, was $43,318. This seems a better measure of central\\ntendency than the mean of $60,828, which indicates how much each family would\\nhave if we all shared equally. And the diﬀerence between these two numbers is quite\\nsubstantial. Nevertheless, both numbers are “correct”, as long as you understand\\ntheir meanings.\\nThe median has a very special property called robustness. A sample statistic\\nis “robust” if moving some data tends not to change the value of the statistic. The\\nmedian is highly robust, because you can move nearly all of the upper half and/or\\nlower half of the data values any distance away from the median without changing\\nthe median. More practically, a few very high values or very low values usually\\nhave no eﬀect on the median.\\nA rarely used measure of central tendency is the mode, which is the most likely\\nor frequently occurring value. More commonly we simply use the term “mode”\\nwhen describing whether a distribution has a single peak (unimodal) or two or\\nmore peaks (bimodal or multi-modal). In symmetric, unimodal distributions, the\\nmode equals both the mean and the median. In unimodal, skewed distributions\\nthe mode is on the other side of the median from the mean.\\nIn multi-modal\\ndistributions there is either no unique highest mode, or the highest mode may well\\nbe unrepresentative of the central tendency.\\nThe most common measure of central tendency is the mean.\\nFor\\nskewed distribution or when there is concern about outliers, the me-\\ndian may be preferred.\\n\\n4.2. UNIVARIATE NON-GRAPHICAL EDA\\n69\\n4.2.4\\nSpread\\nSeveral statistics are commonly used as a measure of the spread of a distribu-\\ntion, including variance, standard deviation, and interquartile range. Spread is an\\nindicator of how far away from the center we are still likely to ﬁnd data values.\\nThe variance is a standard measure of spread. It is calculated for a list of\\nnumbers, e.g., the n observations of a particular measurement labeled x1 through\\nxn, based on the n sample deviations (or just “deviations”). Then for any data\\nvalue, xi, the corresponding deviation is (xi −¯x), which is the signed (- for lower\\nand + for higher) distance of the data value from the mean of all of the n data\\nvalues. It is not hard to prove that the sum of all of the deviations of a sample is\\nzero.\\nThe variance of a population is deﬁned as the mean squared deviation (see\\nsection 3.5.2). The sample formula for the variance of observed data conventionally\\nhas n−1 in the denominator instead of n to achieve the property of “unbiasedness”,\\nwhich roughly means that when calculated for many diﬀerent random samples\\nfrom the same population, the average should match the corresponding population\\nquantity (here, σ2). The most commonly used symbol for sample variance is s2,\\nand the formula is\\ns2 =\\nPn\\ni=1(xi −¯x)2\\n(n −1)\\nwhich is essentially the average of the squared deviations, except for dividing by\\nn −1 instead of n. This is a measure of spread, because the bigger the deviations\\nfrom the mean, the bigger the variance gets. (In most cases, squaring is better\\nthan taking the absolute value because it puts special emphasis on highly deviant\\nvalues.) As usual, a sample statistic like s2 is best thought of as a characteristic of\\na particular sample (thus varying from sample to sample) which is used as an esti-\\nmate of the single, ﬁxed, true corresponding parameter value from the population,\\nnamely σ2.\\nAnother (equivalent) way to write the variance formula, which is particularly\\nuseful for thinking about ANOVA is\\ns2 = SS\\ndf\\nwhere SS is “sum of squared deviations”, often loosely called “sum of squares”,\\nand df is “degrees of freedom” (see section 4.6).\\n\\n70\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nBecause of the square, variances are always non-negative, and they have the\\nsomewhat unusual property of having squared units compared to the original data.\\nSo if the random variable of interest is a temperature in degrees, the variance\\nhas units “degrees squared”, and if the variable is area in square kilometers, the\\nvariance is in units of “kilometers to the fourth power”.\\nVariances have the very important property that they are additive for any\\nnumber of diﬀerent independent sources of variation. For example, the variance of\\na measurement which has subject-to-subject variability, environmental variability,\\nand quality-of-measurement variability is equal to the sum of the three variances.\\nThis property is not shared by the “standard deviation”.\\nThe standard deviation is simply the square root of the variance. Therefore\\nit has the same units as the original data, which helps make it more interpretable.\\nThe sample standard deviation is usually represented by the symbol s.\\nFor a\\ntheoretical Gaussian distribution, we learned in the previous chapter that mean\\nplus or minus 1, 2 or 3 standard deviations holds 68.3, 95.4 and 99.7% of the\\nprobability respectively, and this should be approximately true for real data from\\na Normal distribution.\\nThe variance and standard deviation are two useful measures of\\nspread.\\nThe variance is the mean of the squares of the individual\\ndeviations. The standard deviation is the square root of the variance.\\nFor Normally distributed data, approximately 95% of the values lie\\nwithin 2 sd of the mean.\\nA third measure of spread is the\\ninterquartile range. To deﬁne IQR, we\\nﬁrst need to deﬁne the concepts of quartiles. The quartiles of a population or\\na sample are the three values which divide the distribution or observed data into\\neven fourths. So one quarter of the data fall below the ﬁrst quartile, usually written\\nQ1; one half fall below the second quartile (Q2); and three fourths fall below the\\nthird quartile (Q3). The astute reader will realize that half of the values fall above\\nQ2, one quarter fall above Q3, and also that Q2 is a synonym for the median.\\nOnce the quartiles are deﬁned, it is easy to deﬁne the IQR as IQR = Q3 −Q1.\\nBy deﬁnition, half of the values (and speciﬁcally the middle half) fall within an\\ninterval whose width equals the IQR. If the data are more spread out, then the\\nIQR tends to increase, and vice versa.\\n\\n4.2. UNIVARIATE NON-GRAPHICAL EDA\\n71\\nThe IQR is a more robust measure of spread than the variance or standard\\ndeviation. Any number of values in the top or bottom quarters of the data can\\nbe moved any distance from the median without aﬀecting the IQR at all. More\\npractically, a few extreme outliers have little or no eﬀect on the IQR.\\nIn contrast to the IQR, the range of the data is not very robust at all. The\\nrange of a sample is the distance from the minimum value to the maximum value:\\nrange = maximum - minimum. If you collect repeated samples from a population,\\nthe minimum, maximum and range tend to change drastically from sample to\\nsample, while the variance and standard deviation change less, and the IQR least\\nof all.\\nThe minimum and maximum of a sample may be useful for detecting\\noutliers, especially if you know something about the possible reasonable values for\\nyour variable. They often (but certainly not always) can detect data entry errors\\nsuch as typing a digit twice or transposing digits (e.g., entering 211 instead of 21\\nand entering 19 instead of 91 for data that represents ages of senior citizens.)\\nThe IQR has one more property worth knowing: for normally distributed data\\nonly, the IQR approximately equals 4/3 times the standard deviation. This means\\nthat for Gaussian distributions, you can approximate the sd from the IQR by\\ncalculating 3/4 of the IQR.\\nThe interquartile range (IQR) is a robust measure of spread.\\n4.2.5\\nSkewness and kurtosis\\nTwo additional useful univariate descriptors are the skewness and kurtosis of a dis-\\ntribution. Skewness is a measure of asymmetry. Kurtosis is a measure of “peaked-\\nness” relative to a Gaussian shape. Sample estimates of skewness and kurtosis are\\ntaken as estimates of the corresponding population parameters (see section 3.5.3).\\nIf the sample skewness and kurtosis are calculated along with their standard errors,\\nwe can roughly make conclusions according to the following table where e is an\\nestimate of skewness and u is an estimate of kurtosis, and SE(e) and SE(u) are\\nthe corresponding standard errors.\\n\\n72\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nSkewness (e) or kurtosis (u)\\nConclusion\\n−2SE(e) < e < 2SE(e)\\nnot skewed\\ne ≤−2SE(e)\\nnegative skew\\ne ≥2SE(e)\\npositive skew\\n−2SE(u) < u < 2SE(u)\\nnot kurtotic\\nu ≤−2SE(u)\\nnegative kurtosis\\nu ≥2SE(u)\\npositive kurtosis\\nFor a positive skew, values far above the mode are more common than values far\\nbelow, and the reverse is true for a negative skew. When a sample (or distribution)\\nhas positive kurtosis, then compared to a Gaussian distribution with the same\\nvariance or standard deviation, values far from the mean (or median or mode) are\\nmore likely, and the shape of the histogram is peaked in the middle, but with fatter\\ntails. For a negative kurtosis, the peak is sometimes described has having “broader\\nshoulders” than a Gaussian shape, and the tails are thinner, so that extreme values\\nare less likely.\\nSkewness is a measure of asymmetry. Kurtosis is a more subtle mea-\\nsure of peakedness compared to a Gaussian distribution.\\n4.3\\nUnivariate graphical EDA\\nIf we are focusing on data from observation of a single variable on n subjects, i.e.,\\na sample of size n, then in addition to looking at the various sample statistics\\ndiscussed in the previous section, we also need to look graphically at the distribu-\\ntion of the sample. Non-graphical and graphical methods complement each other.\\nWhile the non-graphical methods are quantitative and objective, they do not give\\na full picture of the data; therefore, graphical methods, which are more qualitative\\nand involve a degree of subjective analysis, are also required.\\n4.3.1\\nHistograms\\nThe only one of these techniques that makes sense for categorical data is the\\nhistogram (basically just a barplot of the tabulation of the data). A pie chart\\n\\n4.3. UNIVARIATE GRAPHICAL EDA\\n73\\nis equivalent, but not often used. The concepts of central tendency, spread and\\nskew have no meaning for nominal categorical data. For ordinal categorical data,\\nit sometimes makes sense to treat the data as quantitative for EDA purposes; you\\nneed to use your judgment here.\\nThe most basic graph is the histogram, which is a barplot in which each bar\\nrepresents the frequency (count) or proportion (count/total count) of cases for a\\nrange of values. Typically the bars run vertically with the count (or proportion)\\naxis running vertically. To manually construct a histogram, deﬁne the range of data\\nfor each bar (called a bin), count how many cases fall in each bin, and draw the\\nbars high enough to indicate the count. For the simple data set found in EDA1.dat\\nthe histogram is shown in ﬁgure 4.2. Besides getting the general impression of the\\nshape of the distribution, you can read oﬀfacts like “there are two cases with data\\nvalues between 1 and 2” and “there are 9 cases with data values between 2 and\\n3”. Generally values that fall exactly on the boundary between two bins are put\\nin the lower bin, but this rule is not always followed.\\nGenerally you will choose between about 5 and 30 bins, depending on the\\namount of data and the shape of the distribution.\\nOf course you need to see\\nthe histogram to know the shape of the distribution, so this may be an iterative\\nprocess. It is often worthwhile to try a few diﬀerent bin sizes/numbers because,\\nespecially with small samples, there may sometimes be a diﬀerent shape to the\\nhistogram when the bin size changes. But usually the diﬀerence is small. Figure\\n4.3 shows three histograms of the same sample from a bimodal population using\\nthree diﬀerent bin widths (5, 2 and 1).\\nIf you want to try on your own, the\\ndata are in EDA2.dat. The top panel appears to show a unimodal distribution.\\nThe middle panel correctly shows the bimodality. The bottom panel incorrectly\\nsuggests many modes. There is some art to choosing bin widths, and although\\noften the automatic choices of a program like SPSS are pretty good, they are\\ncertainly not always adequate.\\nIt is very instructive to look at multiple samples from the same population to\\nget a feel for the variation that will be found in histograms. Figure 4.4 shows\\nhistograms from multiple samples of size 50 from the same population as ﬁgure\\n4.3, while 4.5 shows samples of size 100. Notice that the variability is quite high,\\nespecially for the smaller sample size, and that an incorrect impression (particularly\\nof unimodality) is quite possible, just by the bad luck of taking a particular sample.\\n\\n74\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nX\\nFrequency\\n0\\n2\\n4\\n6\\n8\\n10\\n0\\n2\\n4\\n6\\n8\\n10\\nFigure 4.2: Histogram of EDA1.dat.\\n\\n4.3. UNIVARIATE GRAPHICAL EDA\\n75\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n15\\n20\\n25\\n0\\n5\\n15\\n25\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n15\\n20\\n25\\n0\\n5\\n10\\n15\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n15\\n20\\n25\\n0\\n2\\n4\\n6\\n8\\nFigure 4.3: Histograms of EDA2.dat with diﬀerent bin widths.\\n\\n76\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n2\\n4\\n6\\n8\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n2\\n4\\n6\\n8\\n10\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n2\\n4\\n6\\n8\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n2\\n4\\n6\\n8\\n10\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n2\\n4\\n6\\n8\\n12\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n2\\n4\\n6\\n8\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n2\\n4\\n6\\n8\\n10\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n2\\n4\\n6\\n8\\n10\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n2\\n4\\n6\\n8\\n10\\nFigure 4.4: Histograms of multiple samples of size 50.\\n\\n4.3. UNIVARIATE GRAPHICAL EDA\\n77\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n5\\n10\\n15\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n5\\n10\\n15\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n5\\n10\\n15\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n5\\n10\\n15\\n20\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n5\\n10\\n15\\n20\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n5\\n10\\n15\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n5\\n10\\n15\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n5\\n10\\n15\\nX\\nFrequency\\n−5\\n0\\n5\\n10\\n20\\n0\\n5\\n10\\n15\\nFigure 4.5: Histograms of multiple samples of size 100.\\n\\n78\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nWith practice, histograms are one of the best ways to quickly learn\\na lot about your data, including central tendency, spread, modality,\\nshape and outliers.\\n4.3.2\\nStem-and-leaf plots\\nA simple substitute for a histogram is a stem and leaf plot. A stem and leaf\\nplot is sometimes easier to make by hand than a histogram, and it tends not to\\nhide any information. Nevertheless, a histogram is generally considered better for\\nappreciating the shape of a sample distribution than is the stem and leaf plot.\\nHere is a stem and leaf plot for the data of ﬁgure 4.2:\\nThe decimal place is at the \"|\".\\n1|000000\\n2|00\\n3|000000000\\n4|000000\\n5|00000000000\\n6|000\\n7|0000\\n8|0\\n9|00\\nBecause this particular stem and leaf plot has the decimal place at the stem,\\neach of the 0’s in the ﬁrst line represent 1.0, and each zero in the second line\\nrepresents 2.0, etc. So we can see that there are six 1’s, two 2’s etc. in our data.\\nA stem and leaf plot shows all data values and the shape of the dis-\\ntribution.\\n\\n4.3. UNIVARIATE GRAPHICAL EDA\\n79\\nG\\n2\\n4\\n6\\n8\\nX\\nFigure 4.6: A boxplot of the data from EDA1.dat.\\n4.3.3\\nBoxplots\\nAnother very useful univariate graphical technique is the boxplot. The boxplot\\nwill be described here in its vertical format, which is the most common, but a\\nhorizontal format also is possible. An example of a boxplot is shown in ﬁgure 4.6,\\nwhich again represents the data in EDA1.dat.\\nBoxplots are very good at presenting information about the central tendency,\\nsymmetry and skew, as well as outliers, although they can be misleading about\\naspects such as multimodality. One of the best uses of boxplots is in the form of\\nside-by-side boxplots (see multivariate graphical analysis below).\\nFigure 4.7 is an annotated version of ﬁgure 4.6. Here you can see that the\\nboxplot consists of a rectangular box bounded above and below by “hinges” that\\nrepresent the quartiles Q3 and Q1 respectively, and with a horizontal “median”\\n\\n80\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\n2\\n4\\n6\\n8\\nX\\nG\\n2\\n4\\n6\\n8\\nX\\nLower whisker end\\nQ1 or lower hinge\\nMedian\\nQ3 or upper hinge\\nUpper whisker end\\nOutlier\\nLower whisker\\nUpper whisker\\nIQR\\nFigure 4.7: Annotated boxplot.\\n\\n4.3. UNIVARIATE GRAPHICAL EDA\\n81\\nline through it. You can also see the upper and lower “whiskers”, and a point\\nmarking an “outlier”. The vertical axis is in the units of the quantitative variable.\\nLet’s assume that the subjects for this experiment are hens and the data rep-\\nresent the number of eggs that each hen laid during the experiment. We can read\\ncertain information directly oﬀof the graph. The median (not mean!) is 4 eggs,\\nso no more than half of the hens laid more than 4 eggs and no more than half of\\nthe hens laid less than 4 eggs. (This is based on the technical deﬁnition of median;\\nwe would usually claim that half of the hens lay more or half less than 4, knowing\\nthat this may be only approximately correct.) We can also state that one quarter\\nof the hens lay less than 3 eggs and one quarter lay more than 5 eggs (again, this\\nmay not be exactly correct, particularly for small samples or a small number of\\ndiﬀerent possible values). This leaves half of the hens, called the “central half”, to\\nlay between 3 and 5 eggs, so the interquartile range (IQR) is Q3-Q1=5-3=2.\\nThe interpretation of the whiskers and outliers is just a bit more complicated.\\nAny data value more than 1.5 IQRs beyond its corresponding hinge in either\\ndirection is considered an “outlier” and is individually plotted. Sometimes values\\nbeyond 3.0 IQRs are considered “extreme outliers” and are plotted with a diﬀerent\\nsymbol. In this boxplot, a single outlier is plotted corresponding to 9 eggs laid,\\nalthough we know from ﬁgure 4.2 that there are actually two hens that laid 9 eggs.\\nThis demonstrates a general problem with plotting whole number data, namely\\nthat multiple points may be superimposed, giving a wrong impression. (Jittering,\\ncircle plots, and starplots are examples of ways to correct this problem.) This is\\none reason why, e.g., combining a tabulation and/or a histogram with a boxplot\\nis better than either alone.\\nEach whisker is drawn out to the most extreme data point that is less than 1.5\\nIQRs beyond the corresponding hinge. Therefore, the whisker ends correspond to\\nthe minimum and maximum values of the data excluding the “outliers”.\\nImportant: The term “outlier” is not well deﬁned in statistics, and the deﬁnition\\nvaries depending on the purpose and situation.\\nThe “outliers” identiﬁed by a\\nboxplot, which could be called “boxplot outliers” are deﬁned as any points more\\nthan 1.5 IQRs above Q3 or more than 1.5 IQRs below Q1. This does not by itself\\nindicate a problem with those data points. Boxplots are an exploratory technique,\\nand you should consider designation as a boxplot outlier as just a suggestion that\\nthe points might be mistakes or otherwise unusual. Also, points not designated\\nas boxplot outliers may also be mistakes. It is also important to realize that the\\nnumber of boxplot outliers depends strongly on the size of the sample. In fact, for\\n\\n82\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\ndata that is perfectly Normally distributed, we expect 0.70 percent (or about 1 in\\n150 cases) to be “boxplot outliers”, with approximately half in either direction.\\nThe boxplot information described above could be appreciated almost as easily\\nif given in non-graphical format. The boxplot is useful because, with practice, all\\nof the above and more can be appreciated at a quick glance. The additional things\\nyou should notice on the plot are the symmetry of the distribution and possible\\nevidence of “fat tails”. Symmetry is appreciated by noticing if the median is in\\nthe center of the box and if the whiskers are the same length as each other. For\\nthis purpose, as usual, the smaller the dataset the more variability you will see\\nfrom sample to sample, particularly for the whiskers. In a skewed distribution we\\nexpect to see the median pushed in the direction of the shorter whisker. If the\\nlonger whisker is the top one, then the distribution is positively skewed (or skewed\\nto the right, because higher values are on the right in a histogram). If the lower\\nwhisker is longer, the distribution is negatively skewed (or left skewed.) In cases\\nwhere the median is closer to the longer whisker it is hard to draw a conclusion.\\nThe term fat tails is used to describe the situation where a histogram has a lot\\nof values far from the mean relative to a Gaussian distribution. This corresponds\\nto positive kurtosis. In a boxplot, many outliers (more than the 1/150 expected\\nfor a Normal distribution) suggests fat tails (positive kurtosis), or possibly many\\ndata entry errors. Also, short whiskers suggest negative kurtosis, at least if the\\nsample size is large.\\nBoxplots are excellent EDA plots because they rely on robust statistics like\\nmedian and IQR rather than more sensitive ones such as mean and standard devi-\\nation. With boxplots it is easy to compare distributions (usually for one variable\\nat diﬀerent levels of another; see multivariate graphical EDA, below) with a high\\ndegree of reliability because of the use of these robust statistics.\\nIt is worth noting that some (few) programs produce boxplots that do not\\nconform to the deﬁnitions given here.\\nBoxplots show robust measures of location and spread as well as pro-\\nviding information about symmetry and outliers.\\n\\n4.3. UNIVARIATE GRAPHICAL EDA\\n83\\nFigure 4.8: A quantile-normal plot.\\n4.3.4\\nQuantile-normal plots\\nThe ﬁnal univariate graphical EDA technique is the most complicated. It is called\\nthe quantile-normal or QN plot or more generality the quantile-quantile\\nor QQ plot. It is used to see how well a particular sample follows a particular\\ntheoretical distribution. Although it can be used for any theoretical distribution,\\nwe will limit our attention to seeing how well a sample of data of size n matches\\na Gaussian distribution with mean and variance equal to the sample mean and\\nvariance. By examining the quantile-normal plot we can detect left or right skew,\\npositive or negative kurtosis, and bimodality.\\nThe example shown in ﬁgure 4.8 shows 20 data points that are approximately\\nnormally distributed. Do not confuse a quantile-normal plot with a simple\\nscatter plot of two variables. The title and axis labels are strong indicators that\\nthis is a quantile-normal plot. For many computer programs, the word “quantile”\\nis also in the axis labels.\\nMany statistical tests have the assumption that the outcome for any ﬁxed set\\nof values of the explanatory variables is approximately normally distributed, and\\nthat is why QN plots are useful: if the assumption is grossly violated, the p-value\\nand conﬁdence intervals of those tests are wrong. As we will see in the ANOVA\\nand regression chapters, the most important situation where we use a QN plot is\\nnot for EDA, but for examining something called “residuals” (see section 9.4). For\\n\\n84\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nbasic interpretation of the QN plot you just need to be able to distinguish the two\\nsituations of “OK” (points fall randomly around the line) versus “non-normality”\\n(points follow a strong curved pattern rather than following the line).\\nIf you are still curious, here is a description of how the QN plot is\\ncreated. Understanding this will help to understand the interpretation,\\nbut is not required in this course. Note that some programs swap the x\\nand y axes from the way described here, but the interpretation is similar\\nfor all versions of QN plots. Consider the 20 values observed in this study.\\nThey happen to have an observed mean of 1.37 and a standard deviation of\\n1.36. Ideally, 20 random values drawn from a distribution that has a true\\nmean of 1.37 and sd of 1.36 have a perfect bell-shaped distribution and\\nwill be spaced so that there is equal area (probability) in the area around\\neach value in the bell curve.\\nIn ﬁgure 4.9 the dotted lines divide the bell curve up into 20 equally\\nprobable zones, and the 20 points are at the probability mid-points of each\\nzone. These 20 points, which are more tightly packed near the middle than\\nin the ends, are used as the “Expected Normal Values” in the QN plot of\\nour actual data.\\nIn summary, the sorted actual data values are plotted against “Ex-\\npected Normal Values”, and some kind of diagonal line is added to help\\ndirect the eye towards a perfect straight line on the quantile-normal plot\\nthat represents a perfect bell shape for the observed data.\\nThe interpretation of the QN plot is given here. If the axes are reversed in\\nthe computer package you are using, you will need to correspondingly change your\\ninterpretation. If all of the points fall on or nearly on the diagonal line (with a\\nrandom pattern), this tells us that a histogram of the variable will show a bell\\nshaped (Normal or Gaussian) distribution.\\nFigure 4.10 shows all of the points basically on the reference line, but there\\nare several vertical bands of points. Because the x-axis is “observed values”, these\\nbands indicate ties, i.e., multiple points with the same values.\\nAnd all of the\\nobserved values are at whole numbers. So either the data are rounded or we are\\nlooking at a discrete quantitative (counting) variable. Either way, the data appear\\n\\n4.3. UNIVARIATE GRAPHICAL EDA\\n85\\n−2\\n0\\n2\\n4\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\nExpected Normal Value\\nDensity\\nG\\nG\\nG G G G GGGGGGGG G G G G\\nG\\nG\\nFigure 4.9: A way to think about QN plots.\\n\\n86\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nFigure 4.10: Quantile-normal plot with ties.\\nto be nearly normally distributed.\\nIn ﬁgure 4.11 note that we have many points in a row that are on the same\\nside of the line (rather than just bouncing around to either side), and that suggests\\nthat there is a real (non-random) deviation from Normality. The best way to think\\nabout these QN plots is to look at the low and high ranges of the Expected Normal\\nValues. In each area, see how the observed values deviate from what is expected,\\ni.e., in which “x” (Observed Value) direction the points appear to have moved\\nrelative to the “perfect normal” line. Here we observe values that are too high in\\nboth the low and high ranges. So compared to a perfect bell shape, this distribution\\nis pulled asymmetrically towards higher values, which indicates positive skew.\\nAlso note that if you just shift a distribution to the right (without disturbing\\nits symmetry) rather than skewing it, it will maintain its perfect bell shape, and\\nthe points remain on the diagonal reference line of the quantile-normal curve.\\nOf course, we can also have a distribution that is skewed to the left, in which\\ncase the high and low range points are shifted (in the Observed Value direction)\\ntowards lower than expected values.\\nIn ﬁgure 4.12 the high end points are shifted too high and the low end points\\nare shifted too low. These data show a positive kurtosis (fat tails). The opposite\\npattern is a negative kurtosis in which the tails are too “thin” to be bell shaped.\\n\\n4.3. UNIVARIATE GRAPHICAL EDA\\n87\\nFigure 4.11: Quantile-normal plot showing right skew.\\nFigure 4.12: Quantile-normal plot showing fat tails.\\n\\n88\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nFigure 4.13: Quantile-normal plot showing a high outlier.\\nIn ﬁgure 4.13 there is a single point that is oﬀthe reference line, i.e. shifted\\nto the right of where it should be. (Remember that the pattern of locations on\\nthe Expected Normal Value axis is ﬁxed for any sample size, and only the position\\non the Observed axis varies depending on the observed data.) This pattern shows\\nnearly Gaussian data with one “high outlier”.\\nFinally, ﬁgure 4.14 looks a bit similar to the “skew left” pattern, but the most\\nextreme points tend to return to the reference line. This pattern is seen in bi-modal\\ndata, e.g. this is what we would see if we would mix strength measurements from\\ncontrols and muscular dystrophy patients.\\nQuantile-Normal plots allow detection of non-normality and diagnosis\\nof skewness and kurtosis.\\n4.4\\nMultivariate non-graphical EDA\\nMultivariate non-graphical EDA techniques generally show the relationship be-\\ntween two or more variables in the form of either cross-tabulation or statistics.\\n\\n4.4. MULTIVARIATE NON-GRAPHICAL EDA\\n89\\nFigure 4.14: Quantile-normal plot showing bimodality.\\n4.4.1\\nCross-tabulation\\nFor categorical data (and quantitative data with only a few diﬀerent values) an\\nextension of tabulation called cross-tabulation is very useful. For two variables,\\ncross-tabulation is performed by making a two-way table with column headings\\nthat match the levels of one variable and row headings that match the levels of\\nthe other variable, then ﬁlling in the counts of all subjects that share a pair of\\nlevels. The two variables might be both explanatory, both outcome, or one of\\neach. Depending on the goals, row percentages (which add to 100% for each row),\\ncolumn percentages (which add to 100% for each column) and/or cell percentages\\n(which add to 100% over all cells) are also useful.\\nHere is an example of a cross-tabulation. Consider the data in table 4.1. For\\neach subject we observe sex and age as categorical variables.\\nTable 4.2 shows the cross-tabulation.\\nWe can easily see that the total number of young females is 2, and we can\\ncalculate, e.g., the corresponding cell percentage is 2/11 × 100 = 18.2%, the row\\npercentage is 2/5×100 = 40.0%, and the column percentage is 2/7×100 = 28.6%.\\nCross-tabulation can be extended to three (and sometimes more) variables by\\nmaking separate two-way tables for two variables at each level of a third variable.\\n\\n90\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nSubject ID\\nAge Group\\nSex\\nGW\\nyoung\\nF\\nJA\\nmiddle\\nF\\nTJ\\nyoung\\nM\\nJMA\\nyoung\\nM\\nJMO\\nmiddle\\nF\\nJQA\\nold\\nF\\nAJ\\nold\\nF\\nMVB\\nyoung\\nM\\nWHH\\nold\\nF\\nJT\\nyoung\\nF\\nJKP\\nmiddle\\nM\\nTable 4.1: Sample Data for Cross-tabulation\\nAge Group / Sex\\nFemale\\nMale\\nTotal\\nyoung\\n2\\n3\\n5\\nmiddle\\n2\\n1\\n3\\nold\\n3\\n0\\n3\\nTotal\\n7\\n4\\n11\\nTable 4.2: Cross-tabulation of Sample Data\\nFor example, we could make separate age by gender tables for each education level.\\nCross-tabulation is the basic bivariate non-graphical EDA technique.\\n4.4.2\\nCorrelation for categorical data\\nAnother statistic that can be calculated for two categorical variables is their corre-\\nlation. But there are many forms of correlation for categorical variables, and that\\nmaterial is currently beyond the scope of this book.\\n\\n4.4. MULTIVARIATE NON-GRAPHICAL EDA\\n91\\n4.4.3\\nUnivariate statistics by category\\nFor one categorical variable (usually explanatory) and one quantitative variable\\n(usually outcome), it is common to produce some of the standard univariate non-\\ngraphical statistics for the quantitative variables separately for each level of the\\ncategorical variable, and then compare the statistics across levels of the categorical\\nvariable.\\nComparing the means is an informal version of ANOVA. Comparing\\nmedians is a robust informal version of one-way ANOVA. Comparing measures of\\nspread is a good informal test of the assumption of equal variances needed for valid\\nanalysis of variance.\\nEspecially for a categorical explanatory variable and a quantitative\\noutcome variable, it is useful to produce a variety of univariate statis-\\ntics for the quantitative variable at each level of the categorical vari-\\nable.\\n4.4.4\\nCorrelation and covariance\\nFor two quantitative variables, the basic statistics of interest are the sample co-\\nvariance and/or sample correlation, which correspond to and are estimates of the\\ncorresponding population parameters from section 3.5. The sample covariance is\\na measure of how much two variables “co-vary”, i.e., how much (and in what\\ndirection) should we expect one variable to change when the other changes.\\nSample covariance is calculated by computing (signed) deviations of\\neach measurement from the average of all measurements for that variable.\\nThen the deviations for the two measurements are multiplied together sepa-\\nrately for each subject. Finally these values are averaged (actually summed\\nand divided by n-1, to keep the statistic unbiased). Note that the units on\\nsample covariance are the products of the units of the two variables.\\nPositive covariance values suggest that when one measurement is above the\\nmean the other will probably also be above the mean, and vice versa. Negative\\n\\n92\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\ncovariances suggest that when one variable is above its mean, the other is below its\\nmean. And covariances near zero suggest that the two variables vary independently\\nof each other.\\nTechnically, independence implies zero correlation, but the reverse is\\nnot necessarily true.\\nCovariances tend to be hard to interpret, so we often use correlation instead.\\nThe correlation has the nice property that it is always between -1 and +1, with\\n-1 being a “perfect” negative linear correlation, +1 being a perfect positive linear\\ncorrelation and 0 indicating that X and Y are uncorrelated. The symbol r or rx,y\\nis often used for sample correlations.\\nThe general formula for sample covariance is\\nCov(X, Y ) =\\nPn\\ni=1(xi −¯x)(yi −¯y)\\nn −1\\nIt is worth noting that Cov(X, X) = Var(X).\\nIf you want to see a “manual example” of calculation of sample covari-\\nance and correlation consider an example using the data in table 4.3. For\\neach subject we observe age and a strength measure.\\nTable 4.4 shows the calculation of covariance. The mean age is 50 and\\nthe mean strength is 19, so we calculate the deviation for age as age-50\\nand deviation for strength and strength-19. Then we ﬁnd the product of\\nthe deviations and add them up. This total is 1106, and since n=11, the\\ncovariance of x and y is -1106/10=-110.6. The fact that the covariance is\\nnegative indicates that as age goes up strength tends to go down (and vice\\nversa).\\nThe formula for the sample correlation is\\nCor(X, Y ) = Cov(X, Y )\\nsxsy\\n\\n4.4. MULTIVARIATE NON-GRAPHICAL EDA\\n93\\nwhere sx is the standard deviation of X and sy is the standard deviation\\nof Y .\\nIn this example, sx = 18.96, sy = 6.39, so r =\\n−110.6\\n18.96·6.39 = −0.913. This\\nis a strong negative correlation.\\nSubject ID\\nAge\\nStrength\\nGW\\n38\\n20\\nJA\\n62\\n15\\nTJ\\n22\\n30\\nJMA\\n38\\n21\\nJMO\\n45\\n18\\nJQA\\n69\\n12\\nAJ\\n75\\n14\\nMVB\\n38\\n28\\nWHH\\n80\\n9\\nJT\\n32\\n22\\nJKP\\n51\\n20\\nTable 4.3: Covariance Sample Data\\n4.4.5\\nCovariance and correlation matrices\\nWhen we have many quantitative variables the most common non-graphical EDA\\ntechnique is to calculate all of the pairwise covariances and/or correlations and\\nassemble them into a matrix. Note that the covariance of X with X is the variance\\nof X and the correlation of X with X is 1.0. For example the covariance matrix\\nof table 4.5 tells us that the variances of X, Y , and Z are 5, 7, and 4 respectively,\\nthe covariance of X and Y is 1.77, the covariance of X and Z is -2.24, and the\\ncovariance of Y and Z is 3.17.\\nSimilarly the correlation matrix in ﬁgure 4.6 tells us that the correlation of X\\nand Y is 0.3, the correlation of X and Z is -0.5. and the correlation of Y and Z\\nis 0.6.\\n\\n94\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nSubject ID\\nAge\\nStrength\\nAge-50\\nStr-19\\nproduct\\nGW\\n38\\n20\\n-12\\n+1\\n-12\\nJA\\n62\\n15\\n+12\\n-4\\n-48\\nTJ\\n22\\n30\\n-28\\n+11\\n-308\\nJMA\\n38\\n21\\n-12\\n+2\\n-24\\nJMO\\n45\\n18\\n-5\\n-1\\n+5\\nJQA\\n69\\n12\\n+19\\n-7\\n-133\\nAJ\\n75\\n14\\n+25\\n-5\\n-125\\nMVB\\n38\\n28\\n-12\\n+9\\n-108\\nWHH\\n80\\n9\\n+30\\n-10\\n-300\\nJT\\n32\\n22\\n-18\\n+3\\n-54\\nJKP\\n51\\n20\\n+1\\n+1\\n+1\\nTotal\\n0\\n0\\n-1106\\nTable 4.4: Covariance Calculation\\nX\\nY\\nZ\\nX\\n5.00\\n1.77\\n-2.24\\nY\\n1.77\\n7.0\\n3.17\\nZ\\n-2.24\\n3.17\\n4.0\\nTable 4.5: A Covariance Matrix\\nThe correlation between two random variables is a number that runs\\nfrom -1 through 0 to +1 and indicates a strong inverse relationship,\\nno relationship, and a strong direct relationship, respectively.\\n4.5\\nMultivariate graphical EDA\\nThere are few useful techniques for graphical EDA of two categorical random\\nvariables. The only one used commonly is a grouped barplot with each group rep-\\nresenting one level of one of the variables and each bar within a group representing\\nthe levels of the other variable.\\n\\n4.5. MULTIVARIATE GRAPHICAL EDA\\n95\\nX\\nY\\nZ\\nX\\n1.0\\n0.3\\n-0.5\\nY\\n0.3\\n1.0\\n0.6\\nZ\\n-0.5\\n0.6\\n1.0\\nTable 4.6: A Correlation Matrix\\n4.5.1\\nUnivariate graphs by category\\nWhen we have one categorical (usually explanatory) and one quantitative (usually\\noutcome) variable, graphical EDA usually takes the form of “conditioning” on\\nthe categorical random variable. This simply indicates that we focus on all of\\nthe subjects with a particular level of the categorical random variable, then make\\nplots of the quantitative variable for those subjects. We repeat this for each level\\nof the categorical variable, then compare the plots. The most commonly used of\\nthese are side-by-side boxplots, as in ﬁgure 4.15. Here we see the data from\\nEDA3.dat, which consists of strength data for each of three age groups. You can\\nsee the downward trend in the median as the ages increase. The spreads (IQRs)\\nare similar for the three groups. And all three groups are roughly symmetrical\\nwith one high strength outlier in the youngest age group.\\nSide-by-side boxplots are the best graphical EDA technique for exam-\\nining the relationship between a categorical variable and a quantitative\\nvariable, as well as the distribution of the quantitative variable at each\\nlevel of the categorical variable.\\n4.5.2\\nScatterplots\\nFor two quantitative variables, the basic graphical EDA technique is the scatterplot\\nwhich has one variable on the x-axis, one on the y-axis and a point for each case\\nin your dataset. If one variable is explanatory and the other is outcome, it is a\\nvery, very strong convention to put the outcome on the y (vertical) axis.\\nOne or two additional categorical variables can be accommodated on the scat-\\nterplot by encoding the additional information in the symbol type and/or color.\\n\\n96\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\nG\\n(21,42]\\n(42,62]\\n(62,82]\\n10\\n15\\n20\\n25\\n30\\n35\\nAge Group\\nStrength\\nFigure 4.15: Side-by-side Boxplot of EDA3.dat.\\n\\n4.5. MULTIVARIATE GRAPHICAL EDA\\n97\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\nG\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n10\\n15\\n20\\n25\\n30\\n35\\nAge\\nStrength\\nG\\nG\\nF/Dem\\nF/Rep\\nM/Dem\\nM/Rep\\nFigure 4.16: scatterplot with two additional variables.\\nAn example is shown in ﬁgure 4.16. Age vs. strength is shown, and diﬀerent colors\\nand symbols are used to code political party and gender.\\nIn a nutshell: You should always perform appropriate EDA before\\nfurther analysis of your data. Perform whatever steps are necessary\\nto become more familiar with your data, check for obvious mistakes,\\nlearn about variable distributions, and learn about relationships be-\\ntween variables. EDA is not an exact science – it is a very important\\nart!\\n\\n98\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\n4.6\\nA note on degrees of freedom\\nDegrees of freedom are numbers that characterize speciﬁc distributions in a family\\nof distributions. Often we ﬁnd that a certain family of distributions is needed in\\na some general situation, and then we need to calculate the degrees of freedom to\\nknow which speciﬁc distribution within the family is appropriate.\\nThe most common situation is when we have a particular statistic and want to\\nknow its sampling distribution. If the sampling distribution falls in the “t” family\\nas when performing a t-test, or in the “F” family when performing an ANOVA,\\nor in several other families, we need to ﬁnd the number of degrees of freedom to\\nﬁgure out which particular member of the family actually represents the desired\\nsampling distribution. One way to think about degrees of freedom for a statistic is\\nthat they represent the number of independent pieces of information that go into\\nthe calculation of the statistic,\\nConsider 5 numbers with a mean of 10. To calculate the variance of these\\nnumbers we need to sum the squared deviations (from the mean). It really doesn’t\\nmatter whether the mean is 10 or any other number: as long as all ﬁve deviations\\nare the same, the variance will be the same. This make sense because variance is a\\npure measure of spread, not aﬀected by central tendency. But by mathematically\\nrearranging the deﬁnition of mean, it is not too hard to show that the sum of\\nthe deviations (not squared) is always zero. Therefore, the ﬁrst four deviations\\ncan (freely) be any numbers, but then the last one is forced to be the number\\nthat makes the deviations add to zero, and we are not free to choose it. It is in\\nthis sense that ﬁve numbers used for calculating a variance or standard deviation\\nhave only four degrees of freedom (or independent useful pieces of information).\\nIn general, a variance or standard deviation calculated from n data values and one\\nmean has n −1 df.\\nAnother example is the “pooled” variance from k independent groups. If the\\nsizes of the groups are n1 through nk, then each of the k individual variance\\nestimates is based on deviations from a diﬀerent mean, and each has one less\\ndegree of freedom than its sample size, e.g., ni −1 for group i. We also say that\\neach numerator of a variance estimate, e.g., SSi, has ni−1 df. The pooled estimate\\nof variance is\\ns2\\npooled = SS1 + · · · + SSk\\ndf1 + · · · + dfk\\nand we say that both the numerator SS and the entire pooled variance has df1+· · ·+\\n\\n4.6. A NOTE ON DEGREES OF FREEDOM\\n99\\ndfk degrees of freedom, which suggests how many independent pieces of information\\nare available for the calculation.\\n\\n100\\nCHAPTER 4. EXPLORATORY DATA ANALYSIS\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "unclean_text = extract_text_from_pdf(eda_chatper)\n",
    "unclean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text_(text):\n",
    "    # text = text.replace(\"\\n\", \" \")  # Replace newlines with spaces\n",
    "    text = re.sub(r'\\s+', ' ', str(text))  # Remove extra spaces\n",
    "    return text.strip()  # Trim leading and trailing spaces\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,!?\\'\" ]', '', text)  # Keep letters, numbers, and common punctuation\n",
    "    return text\n",
    "\n",
    "def fix_hyphenation(text):\n",
    "    return re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)  # Removes hyphenation across lines\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "def remove_headers_footers(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = [line for line in lines if not re.match(r'(Page \\d+|Confidential|Company Name)', line)]\n",
    "    return \" \".join(cleaned_lines)\n",
    "\n",
    "def normalize_text(text):\n",
    "    return \" \".join(text.lower().split())\n",
    "\n",
    "def full_text_cleanup(text):\n",
    "    \"\"\"\"\n",
    "    Takes in unclean text and return cleaned text by applying a series of cleaning functions.\n",
    "    \n",
    "    \"\"\"\n",
    "    text = clean_text_(text)\n",
    "    text = fix_hyphenation(text)\n",
    "    text = remove_special_chars(text)\n",
    "    text = normalize_unicode(text)\n",
    "    text = remove_headers_footers(text)\n",
    "    text = normalize_text(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chapter 4 exploratory data analysis a rst look at the data. as mentioned in chapter 1, exploratory data analysis or eda is a critical rst step in analyzing the data from an experiment. here are the main reasons we use eda detection of mistakes checking of assumptions preliminary selection of appropriate models determining relationships among the explanatory variables, and assessing the direction and rough size of relationships between explanatory and outcome variables. loosely speaking, any method of looking at data that does not include formal statistical modeling and inference falls under the term exploratory data analysis. 4.1 typical data format and the types of eda the data from an experiment are generally collected into a rectangular array e.g., spreadsheet or database, most commonly with one row per experimental subject 61 62 chapter 4. exploratory data analysis and one column for each subject identier, outcome variable, and explanatory variable. each column contains the numeric values for a particular quantitative variable or the levels for a categorical variable. some more complicated experiments require a more complex data layout. people are not very good at looking at a column of numbers or a whole spreadsheet and then determining important characteristics of the data. they nd looking at numbers to be tedious, boring, andor overwhelming. exploratory data analysis techniques have been devised as an aid in this situation. most of these techniques work in part by hiding certain aspects of the data while making other aspects more clear. exploratory data analysis is generally crossclassied in two ways. first, each method is either nongraphical or graphical. and second, each method is either univariate or multivariate usually just bivariate. nongraphical methods generally involve calculation of summary statistics, while graphical methods obviously summarize the data in a diagrammatic or pictorial way. univariate methods look at one variable data column at a time, while multivariate methods look at two or more variables at a time to explore relationships. usually our multivariate eda will be bivariate looking at exactly two variables, but occasionally it will involve three or more variables. it is almost always a good idea to perform univariate eda on each of the components of a multivariate eda before performing the multivariate eda. beyond the four categories created by the above crossclassication, each of the categories of eda have further divisions based on the role outcome or explanatory and type categorical or quantitative of the variables being examined. although there are guidelines about which eda techniques are useful in what circumstances, there is an important degree of looseness and art to eda. competence and condence come with practice, experience, and close observation of others. also, eda need not be restricted to techniques you have seen before sometimes you need to invent a new way of looking at your data. the four types of eda are univariate nongraphical, multivariate nongraphical, univariate graphical, and multivariate graphical. this chapter rst discusses the nongraphical and graphical methods for looking 4.2. univariate nongraphical eda 63 at single variables, then moves on to looking at multiple variables at once, mostly to investigate the relationships between the variables. 4.2 univariate nongraphical eda the data that come from making a particular measurement on all of the subjects in a sample represent our observations for a single characteristic such as age, gender, speed at a task, or response to a stimulus. we should think of these measurements as representing a sample distribution of the variable, which in turn more or less represents the population distribution of the variable. the usual goal of univariate nongraphical eda is to better appreciate the sample distribution and also to make some tentative conclusions about what population distributions isare compatible with the sample distribution. outlier detection is also a part of this analysis. 4.2.1 categorical data the characteristics of interest for a categorical variable are simply the range of values and the frequency or relative frequency of occurrence for each value. for ordinal variables it is sometimes appropriate to treat them as quantitative variables using the techniques in the second part of this section. therefore the only useful univariate nongraphical techniques for categorical variables is some form of tabulation of the frequencies, usually along with calculation of the fraction or percent of data that falls in each category. for example if we categorize subjects by college at carnegie mellon university as hss, mcs, scs and other, then there is a true population of all students enrolled in the 2007 fall semester. if we take a random sample of 20 students for the purposes of performing a memory experiment, we could list the sample measurements as hss, hss, mcs, other, other, scs, mcs, other, hss, mcs, scs, scs, other, mcs, mcs, hss, mcs, other, hss, scs. our eda would look like this statisticcollege hss mcs scs other total count 5 6 4 5 20 proportion 0.25 0.30 0.20 0.25 1.00 percent 25 30 20 25 100 note that it is useful to have the total count frequency to verify that we 64 chapter 4. exploratory data analysis have an observation for each subject that we recruited. losing data is a common mistake, and eda is very helpful for nding mistakes.. also, we should expect that the proportions add up to 1.00 or 100 if we are calculating them correctly counttotal. once you get used to it, you wont need both proportion relative frequency and percent, because they will be interchangeable in your mind. a simple tabulation of the frequency of each category is the best univariate nongraphical eda for categorical data. 4.2.2 characteristics of quantitative data univariate eda for a quantitative variable is a way to make preliminary assessments about the population distribution of the variable using the data of the observed sample. the characteristics of the population distribution of a quantitative variable are its center, spread, modality number of peaks in the pdf, shape including heaviness of the tails, and outliers. see section 3.5. our observed data represent just one sample out of an innite number of possible samples. the characteristics of our randomly observed sample are not inherently interesting, except to the degree that they represent the population that it came from. what we observe in the sample of measurements for a particular variable that we select for our particular experiment is the sample distribution. we need to recognize that this would be dierent each time we might repeat the same experiment, due to selection of a dierent random sample, a dierent treatment randomization, and dierent random incompletely controlled experimental conditions. in addition we can calculate sample statistics from the data, such as sample mean, sample variance, sample standard deviation, sample skewness and sample kurtosis. these again would vary for each repetition of the experiment, so they dont represent any deep truth, but rather represent some uncertain information about the underlying population distribution and its parameters, which are what we really care about. 4.2. univariate nongraphical eda 65 many of the samples distributional characteristics are seen qualitatively in the univariate graphical eda technique of a histogram see 4.3.1. in most situations it is worthwhile to think of univariate nongraphical eda as telling you about aspects of the histogram of the distribution of the variable of interest. again, these aspects are quantitative, but because they refer to just one of many possible samples from a population, they are best thought of as random nonxed estimates of the xed, unknown parameters see section 3.5 of the distribution of the population of interest. if the quantitative variable does not have too many distinct values, a tabulation, as we used for categorical data, will be a worthwhile univariate, nongraphical technique. but mostly, for quantitative variables we are concerned here with the quantitative numeric nongraphical measures which are the various sample statistics. in fact, sample statistics are generally thought of as estimates of the corresponding population parameters. figure 4.1 shows a histogram of a sample of size 200 from the innite population characterized by distribution c of gure 3.1 from section 3.5. remember that in that section we examined the parameters that characterize theoretical population distributions. now we are interested in learning what we can but not everything, because parameters are secrets of nature about these parameters from measurements on a random sample of subjects out of that population. the bimodality is visible, as is an outlier at x2. there is no generally recognized formal denition for outlier, but roughly it means values that are outside of the areas of a distribution that would commonly occur. this can also be thought of as sample data values which correspond to areas of the population pdf or pmf with low density or probability. the denition of outlier for standard boxplots is described below see 4.3.3. another common denition of outlier consider any point more than a xed number of standard deviations from the mean to be an outlier, but these and other denitions are arbitrary and vary from situation to situation. for quantitative variables and possibly for ordinal variables it is worthwhile looking at the central tendency, spread, skewness, and kurtosis of the data for a particular variable from an experiment. but for categorical variables, none of these make any sense. 66 chapter 4. exploratory data analysis x frequency 2 1 0 1 2 3 4 5 0 5 10 15 20 figure 4.1 histogram from distribution c. 4.2. univariate nongraphical eda 67 4.2.3 central tendency the central tendency or location of a distribution has to do with typical or middle values. the common, useful measures of central tendency are the statistics called arithmetic mean, median, and sometimes mode. occasionally other means such as geometric, harmonic, truncated, or winsorized means are used as measures of centrality. while most authors use the term average as a synonym for arithmetic mean, some use average in a broader sense to also include geometric, harmonic, and other means. assuming that we have n data values labeled x1 through xn, the formula for calculating the sample arithmetic mean is x pn i1 xi n . the arithmetic mean is simply the sum of all of the data values divided by the number of values. it can be thought of as how much each subject gets in a fair redivision of whatever the data are measuring. for instance, the mean amount of money that a group of people have is the amount each would get if all of the money were put in one pot, and then the money was redistributed to all people evenly. i hope you can see that this is the same as summing then dividing by n. for any symmetrically shaped distribution i.e., one with a symmetric histogram or pdf or pmf the mean is the point around which the symmetry holds. for nonsymmetric distributions, the mean is the balance point if the histogram is cut out of some homogeneous stimaterial such as cardboard, it will balance on a fulcrum placed at the mean. for many descriptive quantities, there are both a sample and a population version. for a xed nite population or for a theoretic innite population described by a pmf or pdf, there is a single population mean which is a xed, often unknown, value called the mean parameter see section 3.5. on the other hand, the sample mean will vary from sample to sample as dierent samples are taken, and so is a random variable. the probability distribution of the sample mean is referred to as its sampling distribution. this term expresses the idea that any experiment could at least theoretically, given enough resources be repeated many times and various statistics such as the sample mean can be calculated each time. often we can use probability theory to work out the exact distribution of the sample statistic, at least under certain assumptions. the median is another measure of central tendency. the sample median is 68 chapter 4. exploratory data analysis the middle value after all of the values are put in an ordered list. if there are an even number of values, take the average of the two middle values. if there are ties at the middle, some special adjustments are made by the statistical software we will use. in unusual situations for discrete random variables, there may not be a unique median. for symmetric distributions, the mean and the median coincide. for unimodal skewed asymmetric distributions, the mean is farther in the direction of the pulled out tail of the distribution than the median is. therefore, for many cases of skewed distributions, the median is preferred as a measure of central tendency. for example, according to the us census bureau 2004 economic survey, the median income of us families, which represents the income above and below which half of families fall, was 43,318. this seems a better measure of central tendency than the mean of 60,828, which indicates how much each family would have if we all shared equally. and the dierence between these two numbers is quite substantial. nevertheless, both numbers are correct, as long as you understand their meanings. the median has a very special property called robustness. a sample statistic is robust if moving some data tends not to change the value of the statistic. the median is highly robust, because you can move nearly all of the upper half andor lower half of the data values any distance away from the median without changing the median. more practically, a few very high values or very low values usually have no eect on the median. a rarely used measure of central tendency is the mode, which is the most likely or frequently occurring value. more commonly we simply use the term mode when describing whether a distribution has a single peak unimodal or two or more peaks bimodal or multimodal. in symmetric, unimodal distributions, the mode equals both the mean and the median. in unimodal, skewed distributions the mode is on the other side of the median from the mean. in multimodal distributions there is either no unique highest mode, or the highest mode may well be unrepresentative of the central tendency. the most common measure of central tendency is the mean. for skewed distribution or when there is concern about outliers, the median may be preferred. 4.2. univariate nongraphical eda 69 4.2.4 spread several statistics are commonly used as a measure of the spread of a distribution, including variance, standard deviation, and interquartile range. spread is an indicator of how far away from the center we are still likely to nd data values. the variance is a standard measure of spread. it is calculated for a list of numbers, e.g., the n observations of a particular measurement labeled x1 through xn, based on the n sample deviations or just deviations. then for any data value, xi, the corresponding deviation is xi x, which is the signed for lower and for higher distance of the data value from the mean of all of the n data values. it is not hard to prove that the sum of all of the deviations of a sample is zero. the variance of a population is dened as the mean squared deviation see section 3.5.2. the sample formula for the variance of observed data conventionally has n1 in the denominator instead of n to achieve the property of unbiasedness, which roughly means that when calculated for many dierent random samples from the same population, the average should match the corresponding population quantity here, 2. the most commonly used symbol for sample variance is s2, and the formula is s2 pn i1xi x2 n 1 which is essentially the average of the squared deviations, except for dividing by n 1 instead of n. this is a measure of spread, because the bigger the deviations from the mean, the bigger the variance gets. in most cases, squaring is better than taking the absolute value because it puts special emphasis on highly deviant values. as usual, a sample statistic like s2 is best thought of as a characteristic of a particular sample thus varying from sample to sample which is used as an estimate of the single, xed, true corresponding parameter value from the population, namely 2. another equivalent way to write the variance formula, which is particularly useful for thinking about anova is s2 ss df where ss is sum of squared deviations, often loosely called sum of squares, and df is degrees of freedom see section 4.6. 70 chapter 4. exploratory data analysis because of the square, variances are always nonnegative, and they have the somewhat unusual property of having squared units compared to the original data. so if the random variable of interest is a temperature in degrees, the variance has units degrees squared, and if the variable is area in square kilometers, the variance is in units of kilometers to the fourth power. variances have the very important property that they are additive for any number of dierent independent sources of variation. for example, the variance of a measurement which has subjecttosubject variability, environmental variability, and qualityofmeasurement variability is equal to the sum of the three variances. this property is not shared by the standard deviation. the standard deviation is simply the square root of the variance. therefore it has the same units as the original data, which helps make it more interpretable. the sample standard deviation is usually represented by the symbol s. for a theoretical gaussian distribution, we learned in the previous chapter that mean plus or minus 1, 2 or 3 standard deviations holds 68.3, 95.4 and 99.7 of the probability respectively, and this should be approximately true for real data from a normal distribution. the variance and standard deviation are two useful measures of spread. the variance is the mean of the squares of the individual deviations. the standard deviation is the square root of the variance. for normally distributed data, approximately 95 of the values lie within 2 sd of the mean. a third measure of spread is the interquartile range. to dene iqr, we rst need to dene the concepts of quartiles. the quartiles of a population or a sample are the three values which divide the distribution or observed data into even fourths. so one quarter of the data fall below the rst quartile, usually written q1 one half fall below the second quartile q2 and three fourths fall below the third quartile q3. the astute reader will realize that half of the values fall above q2, one quarter fall above q3, and also that q2 is a synonym for the median. once the quartiles are dened, it is easy to dene the iqr as iqr q3 q1. by denition, half of the values and specically the middle half fall within an interval whose width equals the iqr. if the data are more spread out, then the iqr tends to increase, and vice versa. 4.2. univariate nongraphical eda 71 the iqr is a more robust measure of spread than the variance or standard deviation. any number of values in the top or bottom quarters of the data can be moved any distance from the median without aecting the iqr at all. more practically, a few extreme outliers have little or no eect on the iqr. in contrast to the iqr, the range of the data is not very robust at all. the range of a sample is the distance from the minimum value to the maximum value range maximum minimum. if you collect repeated samples from a population, the minimum, maximum and range tend to change drastically from sample to sample, while the variance and standard deviation change less, and the iqr least of all. the minimum and maximum of a sample may be useful for detecting outliers, especially if you know something about the possible reasonable values for your variable. they often but certainly not always can detect data entry errors such as typing a digit twice or transposing digits e.g., entering 211 instead of 21 and entering 19 instead of 91 for data that represents ages of senior citizens. the iqr has one more property worth knowing for normally distributed data only, the iqr approximately equals 43 times the standard deviation. this means that for gaussian distributions, you can approximate the sd from the iqr by calculating 34 of the iqr. the interquartile range iqr is a robust measure of spread. 4.2.5 skewness and kurtosis two additional useful univariate descriptors are the skewness and kurtosis of a distribution. skewness is a measure of asymmetry. kurtosis is a measure of peakedness relative to a gaussian shape. sample estimates of skewness and kurtosis are taken as estimates of the corresponding population parameters see section 3.5.3. if the sample skewness and kurtosis are calculated along with their standard errors, we can roughly make conclusions according to the following table where e is an estimate of skewness and u is an estimate of kurtosis, and see and seu are the corresponding standard errors. 72 chapter 4. exploratory data analysis skewness e or kurtosis u conclusion 2see e 2see not skewed e 2see negative skew e 2see positive skew 2seu u 2seu not kurtotic u 2seu negative kurtosis u 2seu positive kurtosis for a positive skew, values far above the mode are more common than values far below, and the reverse is true for a negative skew. when a sample or distribution has positive kurtosis, then compared to a gaussian distribution with the same variance or standard deviation, values far from the mean or median or mode are more likely, and the shape of the histogram is peaked in the middle, but with fatter tails. for a negative kurtosis, the peak is sometimes described has having broader shoulders than a gaussian shape, and the tails are thinner, so that extreme values are less likely. skewness is a measure of asymmetry. kurtosis is a more subtle measure of peakedness compared to a gaussian distribution. 4.3 univariate graphical eda if we are focusing on data from observation of a single variable on n subjects, i.e., a sample of size n, then in addition to looking at the various sample statistics discussed in the previous section, we also need to look graphically at the distribution of the sample. nongraphical and graphical methods complement each other. while the nongraphical methods are quantitative and objective, they do not give a full picture of the data therefore, graphical methods, which are more qualitative and involve a degree of subjective analysis, are also required. 4.3.1 histograms the only one of these techniques that makes sense for categorical data is the histogram basically just a barplot of the tabulation of the data. a pie chart 4.3. univariate graphical eda 73 is equivalent, but not often used. the concepts of central tendency, spread and skew have no meaning for nominal categorical data. for ordinal categorical data, it sometimes makes sense to treat the data as quantitative for eda purposes you need to use your judgment here. the most basic graph is the histogram, which is a barplot in which each bar represents the frequency count or proportion counttotal count of cases for a range of values. typically the bars run vertically with the count or proportion axis running vertically. to manually construct a histogram, dene the range of data for each bar called a bin, count how many cases fall in each bin, and draw the bars high enough to indicate the count. for the simple data set found in eda1.dat the histogram is shown in gure 4.2. besides getting the general impression of the shape of the distribution, you can read ofacts like there are two cases with data values between 1 and 2 and there are 9 cases with data values between 2 and 3. generally values that fall exactly on the boundary between two bins are put in the lower bin, but this rule is not always followed. generally you will choose between about 5 and 30 bins, depending on the amount of data and the shape of the distribution. of course you need to see the histogram to know the shape of the distribution, so this may be an iterative process. it is often worthwhile to try a few dierent bin sizesnumbers because, especially with small samples, there may sometimes be a dierent shape to the histogram when the bin size changes. but usually the dierence is small. figure 4.3 shows three histograms of the same sample from a bimodal population using three dierent bin widths 5, 2 and 1. if you want to try on your own, the data are in eda2.dat. the top panel appears to show a unimodal distribution. the middle panel correctly shows the bimodality. the bottom panel incorrectly suggests many modes. there is some art to choosing bin widths, and although often the automatic choices of a program like spss are pretty good, they are certainly not always adequate. it is very instructive to look at multiple samples from the same population to get a feel for the variation that will be found in histograms. figure 4.4 shows histograms from multiple samples of size 50 from the same population as gure 4.3, while 4.5 shows samples of size 100. notice that the variability is quite high, especially for the smaller sample size, and that an incorrect impression particularly of unimodality is quite possible, just by the bad luck of taking a particular sample. 74 chapter 4. exploratory data analysis x frequency 0 2 4 6 8 10 0 2 4 6 8 10 figure 4.2 histogram of eda1.dat. 4.3. univariate graphical eda 75 x frequency 5 0 5 10 15 20 25 0 5 15 25 x frequency 5 0 5 10 15 20 25 0 5 10 15 x frequency 5 0 5 10 15 20 25 0 2 4 6 8 figure 4.3 histograms of eda2.dat with dierent bin widths. 76 chapter 4. exploratory data analysis x frequency 5 0 5 10 20 0 2 4 6 8 x frequency 5 0 5 10 20 0 2 4 6 8 10 x frequency 5 0 5 10 20 0 2 4 6 8 x frequency 5 0 5 10 20 0 2 4 6 8 10 x frequency 5 0 5 10 20 0 2 4 6 8 12 x frequency 5 0 5 10 20 0 2 4 6 8 x frequency 5 0 5 10 20 0 2 4 6 8 10 x frequency 5 0 5 10 20 0 2 4 6 8 10 x frequency 5 0 5 10 20 0 2 4 6 8 10 figure 4.4 histograms of multiple samples of size 50. 4.3. univariate graphical eda 77 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 20 x frequency 5 0 5 10 20 0 5 10 15 20 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 figure 4.5 histograms of multiple samples of size 100. 78 chapter 4. exploratory data analysis with practice, histograms are one of the best ways to quickly learn a lot about your data, including central tendency, spread, modality, shape and outliers. 4.3.2 stemandleaf plots a simple substitute for a histogram is a stem and leaf plot. a stem and leaf plot is sometimes easier to make by hand than a histogram, and it tends not to hide any information. nevertheless, a histogram is generally considered better for appreciating the shape of a sample distribution than is the stem and leaf plot. here is a stem and leaf plot for the data of gure 4.2 the decimal place is at the \"\". 1000000 200 3000000000 4000000 500000000000 6000 70000 80 900 because this particular stem and leaf plot has the decimal place at the stem, each of the 0s in the rst line represent 1.0, and each zero in the second line represents 2.0, etc. so we can see that there are six 1s, two 2s etc. in our data. a stem and leaf plot shows all data values and the shape of the distribution. 4.3. univariate graphical eda 79 g 2 4 6 8 x figure 4.6 a boxplot of the data from eda1.dat. 4.3.3 boxplots another very useful univariate graphical technique is the boxplot. the boxplot will be described here in its vertical format, which is the most common, but a horizontal format also is possible. an example of a boxplot is shown in gure 4.6, which again represents the data in eda1.dat. boxplots are very good at presenting information about the central tendency, symmetry and skew, as well as outliers, although they can be misleading about aspects such as multimodality. one of the best uses of boxplots is in the form of sidebyside boxplots see multivariate graphical analysis below. figure 4.7 is an annotated version of gure 4.6. here you can see that the boxplot consists of a rectangular box bounded above and below by hinges that represent the quartiles q3 and q1 respectively, and with a horizontal median 80 chapter 4. exploratory data analysis 2 4 6 8 x g 2 4 6 8 x lower whisker end q1 or lower hinge median q3 or upper hinge upper whisker end outlier lower whisker upper whisker iqr figure 4.7 annotated boxplot. 4.3. univariate graphical eda 81 line through it. you can also see the upper and lower whiskers, and a point marking an outlier. the vertical axis is in the units of the quantitative variable. lets assume that the subjects for this experiment are hens and the data represent the number of eggs that each hen laid during the experiment. we can read certain information directly oof the graph. the median not mean! is 4 eggs, so no more than half of the hens laid more than 4 eggs and no more than half of the hens laid less than 4 eggs. this is based on the technical denition of median we would usually claim that half of the hens lay more or half less than 4, knowing that this may be only approximately correct. we can also state that one quarter of the hens lay less than 3 eggs and one quarter lay more than 5 eggs again, this may not be exactly correct, particularly for small samples or a small number of dierent possible values. this leaves half of the hens, called the central half, to lay between 3 and 5 eggs, so the interquartile range iqr is q3q1532. the interpretation of the whiskers and outliers is just a bit more complicated. any data value more than 1.5 iqrs beyond its corresponding hinge in either direction is considered an outlier and is individually plotted. sometimes values beyond 3.0 iqrs are considered extreme outliers and are plotted with a dierent symbol. in this boxplot, a single outlier is plotted corresponding to 9 eggs laid, although we know from gure 4.2 that there are actually two hens that laid 9 eggs. this demonstrates a general problem with plotting whole number data, namely that multiple points may be superimposed, giving a wrong impression. jittering, circle plots, and starplots are examples of ways to correct this problem. this is one reason why, e.g., combining a tabulation andor a histogram with a boxplot is better than either alone. each whisker is drawn out to the most extreme data point that is less than 1.5 iqrs beyond the corresponding hinge. therefore, the whisker ends correspond to the minimum and maximum values of the data excluding the outliers. important the term outlier is not well dened in statistics, and the denition varies depending on the purpose and situation. the outliers identied by a boxplot, which could be called boxplot outliers are dened as any points more than 1.5 iqrs above q3 or more than 1.5 iqrs below q1. this does not by itself indicate a problem with those data points. boxplots are an exploratory technique, and you should consider designation as a boxplot outlier as just a suggestion that the points might be mistakes or otherwise unusual. also, points not designated as boxplot outliers may also be mistakes. it is also important to realize that the number of boxplot outliers depends strongly on the size of the sample. in fact, for 82 chapter 4. exploratory data analysis data that is perfectly normally distributed, we expect 0.70 percent or about 1 in 150 cases to be boxplot outliers, with approximately half in either direction. the boxplot information described above could be appreciated almost as easily if given in nongraphical format. the boxplot is useful because, with practice, all of the above and more can be appreciated at a quick glance. the additional things you should notice on the plot are the symmetry of the distribution and possible evidence of fat tails. symmetry is appreciated by noticing if the median is in the center of the box and if the whiskers are the same length as each other. for this purpose, as usual, the smaller the dataset the more variability you will see from sample to sample, particularly for the whiskers. in a skewed distribution we expect to see the median pushed in the direction of the shorter whisker. if the longer whisker is the top one, then the distribution is positively skewed or skewed to the right, because higher values are on the right in a histogram. if the lower whisker is longer, the distribution is negatively skewed or left skewed. in cases where the median is closer to the longer whisker it is hard to draw a conclusion. the term fat tails is used to describe the situation where a histogram has a lot of values far from the mean relative to a gaussian distribution. this corresponds to positive kurtosis. in a boxplot, many outliers more than the 1150 expected for a normal distribution suggests fat tails positive kurtosis, or possibly many data entry errors. also, short whiskers suggest negative kurtosis, at least if the sample size is large. boxplots are excellent eda plots because they rely on robust statistics like median and iqr rather than more sensitive ones such as mean and standard deviation. with boxplots it is easy to compare distributions usually for one variable at dierent levels of another see multivariate graphical eda, below with a high degree of reliability because of the use of these robust statistics. it is worth noting that some few programs produce boxplots that do not conform to the denitions given here. boxplots show robust measures of location and spread as well as providing information about symmetry and outliers. 4.3. univariate graphical eda 83 figure 4.8 a quantilenormal plot. 4.3.4 quantilenormal plots the nal univariate graphical eda technique is the most complicated. it is called the quantilenormal or qn plot or more generality the quantilequantile or qq plot. it is used to see how well a particular sample follows a particular theoretical distribution. although it can be used for any theoretical distribution, we will limit our attention to seeing how well a sample of data of size n matches a gaussian distribution with mean and variance equal to the sample mean and variance. by examining the quantilenormal plot we can detect left or right skew, positive or negative kurtosis, and bimodality. the example shown in gure 4.8 shows 20 data points that are approximately normally distributed. do not confuse a quantilenormal plot with a simple scatter plot of two variables. the title and axis labels are strong indicators that this is a quantilenormal plot. for many computer programs, the word quantile is also in the axis labels. many statistical tests have the assumption that the outcome for any xed set of values of the explanatory variables is approximately normally distributed, and that is why qn plots are useful if the assumption is grossly violated, the pvalue and condence intervals of those tests are wrong. as we will see in the anova and regression chapters, the most important situation where we use a qn plot is not for eda, but for examining something called residuals see section 9.4. for 84 chapter 4. exploratory data analysis basic interpretation of the qn plot you just need to be able to distinguish the two situations of ok points fall randomly around the line versus nonnormality points follow a strong curved pattern rather than following the line. if you are still curious, here is a description of how the qn plot is created. understanding this will help to understand the interpretation, but is not required in this course. note that some programs swap the x and y axes from the way described here, but the interpretation is similar for all versions of qn plots. consider the 20 values observed in this study. they happen to have an observed mean of 1.37 and a standard deviation of 1.36. ideally, 20 random values drawn from a distribution that has a true mean of 1.37 and sd of 1.36 have a perfect bellshaped distribution and will be spaced so that there is equal area probability in the area around each value in the bell curve. in gure 4.9 the dotted lines divide the bell curve up into 20 equally probable zones, and the 20 points are at the probability midpoints of each zone. these 20 points, which are more tightly packed near the middle than in the ends, are used as the expected normal values in the qn plot of our actual data. in summary, the sorted actual data values are plotted against expected normal values, and some kind of diagonal line is added to help direct the eye towards a perfect straight line on the quantilenormal plot that represents a perfect bell shape for the observed data. the interpretation of the qn plot is given here. if the axes are reversed in the computer package you are using, you will need to correspondingly change your interpretation. if all of the points fall on or nearly on the diagonal line with a random pattern, this tells us that a histogram of the variable will show a bell shaped normal or gaussian distribution. figure 4.10 shows all of the points basically on the reference line, but there are several vertical bands of points. because the xaxis is observed values, these bands indicate ties, i.e., multiple points with the same values. and all of the observed values are at whole numbers. so either the data are rounded or we are looking at a discrete quantitative counting variable. either way, the data appear 4.3. univariate graphical eda 85 2 0 2 4 0.00 0.05 0.10 0.15 0.20 0.25 0.30 expected normal value density g g g g g g gggggggg g g g g g g figure 4.9 a way to think about qn plots. 86 chapter 4. exploratory data analysis figure 4.10 quantilenormal plot with ties. to be nearly normally distributed. in gure 4.11 note that we have many points in a row that are on the same side of the line rather than just bouncing around to either side, and that suggests that there is a real nonrandom deviation from normality. the best way to think about these qn plots is to look at the low and high ranges of the expected normal values. in each area, see how the observed values deviate from what is expected, i.e., in which x observed value direction the points appear to have moved relative to the perfect normal line. here we observe values that are too high in both the low and high ranges. so compared to a perfect bell shape, this distribution is pulled asymmetrically towards higher values, which indicates positive skew. also note that if you just shift a distribution to the right without disturbing its symmetry rather than skewing it, it will maintain its perfect bell shape, and the points remain on the diagonal reference line of the quantilenormal curve. of course, we can also have a distribution that is skewed to the left, in which case the high and low range points are shifted in the observed value direction towards lower than expected values. in gure 4.12 the high end points are shifted too high and the low end points are shifted too low. these data show a positive kurtosis fat tails. the opposite pattern is a negative kurtosis in which the tails are too thin to be bell shaped. 4.3. univariate graphical eda 87 figure 4.11 quantilenormal plot showing right skew. figure 4.12 quantilenormal plot showing fat tails. 88 chapter 4. exploratory data analysis figure 4.13 quantilenormal plot showing a high outlier. in gure 4.13 there is a single point that is othe reference line, i.e. shifted to the right of where it should be. remember that the pattern of locations on the expected normal value axis is xed for any sample size, and only the position on the observed axis varies depending on the observed data. this pattern shows nearly gaussian data with one high outlier. finally, gure 4.14 looks a bit similar to the skew left pattern, but the most extreme points tend to return to the reference line. this pattern is seen in bimodal data, e.g. this is what we would see if we would mix strength measurements from controls and muscular dystrophy patients. quantilenormal plots allow detection of nonnormality and diagnosis of skewness and kurtosis. 4.4 multivariate nongraphical eda multivariate nongraphical eda techniques generally show the relationship between two or more variables in the form of either crosstabulation or statistics. 4.4. multivariate nongraphical eda 89 figure 4.14 quantilenormal plot showing bimodality. 4.4.1 crosstabulation for categorical data and quantitative data with only a few dierent values an extension of tabulation called crosstabulation is very useful. for two variables, crosstabulation is performed by making a twoway table with column headings that match the levels of one variable and row headings that match the levels of the other variable, then lling in the counts of all subjects that share a pair of levels. the two variables might be both explanatory, both outcome, or one of each. depending on the goals, row percentages which add to 100 for each row, column percentages which add to 100 for each column andor cell percentages which add to 100 over all cells are also useful. here is an example of a crosstabulation. consider the data in table 4.1. for each subject we observe sex and age as categorical variables. table 4.2 shows the crosstabulation. we can easily see that the total number of young females is 2, and we can calculate, e.g., the corresponding cell percentage is 211 100 18.2, the row percentage is 25100 40.0, and the column percentage is 27100 28.6. crosstabulation can be extended to three and sometimes more variables by making separate twoway tables for two variables at each level of a third variable. 90 chapter 4. exploratory data analysis subject id age group sex gw young f ja middle f tj young m jma young m jmo middle f jqa old f aj old f mvb young m whh old f jt young f jkp middle m table 4.1 sample data for crosstabulation age group sex female male total young 2 3 5 middle 2 1 3 old 3 0 3 total 7 4 11 table 4.2 crosstabulation of sample data for example, we could make separate age by gender tables for each education level. crosstabulation is the basic bivariate nongraphical eda technique. 4.4.2 correlation for categorical data another statistic that can be calculated for two categorical variables is their correlation. but there are many forms of correlation for categorical variables, and that material is currently beyond the scope of this book. 4.4. multivariate nongraphical eda 91 4.4.3 univariate statistics by category for one categorical variable usually explanatory and one quantitative variable usually outcome, it is common to produce some of the standard univariate nongraphical statistics for the quantitative variables separately for each level of the categorical variable, and then compare the statistics across levels of the categorical variable. comparing the means is an informal version of anova. comparing medians is a robust informal version of oneway anova. comparing measures of spread is a good informal test of the assumption of equal variances needed for valid analysis of variance. especially for a categorical explanatory variable and a quantitative outcome variable, it is useful to produce a variety of univariate statistics for the quantitative variable at each level of the categorical variable. 4.4.4 correlation and covariance for two quantitative variables, the basic statistics of interest are the sample covariance andor sample correlation, which correspond to and are estimates of the corresponding population parameters from section 3.5. the sample covariance is a measure of how much two variables covary, i.e., how much and in what direction should we expect one variable to change when the other changes. sample covariance is calculated by computing signed deviations of each measurement from the average of all measurements for that variable. then the deviations for the two measurements are multiplied together separately for each subject. finally these values are averaged actually summed and divided by n1, to keep the statistic unbiased. note that the units on sample covariance are the products of the units of the two variables. positive covariance values suggest that when one measurement is above the mean the other will probably also be above the mean, and vice versa. negative 92 chapter 4. exploratory data analysis covariances suggest that when one variable is above its mean, the other is below its mean. and covariances near zero suggest that the two variables vary independently of each other. technically, independence implies zero correlation, but the reverse is not necessarily true. covariances tend to be hard to interpret, so we often use correlation instead. the correlation has the nice property that it is always between 1 and 1, with 1 being a perfect negative linear correlation, 1 being a perfect positive linear correlation and 0 indicating that x and y are uncorrelated. the symbol r or rx,y is often used for sample correlations. the general formula for sample covariance is covx, y pn i1xi xyi y n 1 it is worth noting that covx, x varx. if you want to see a manual example of calculation of sample covariance and correlation consider an example using the data in table 4.3. for each subject we observe age and a strength measure. table 4.4 shows the calculation of covariance. the mean age is 50 and the mean strength is 19, so we calculate the deviation for age as age50 and deviation for strength and strength19. then we nd the product of the deviations and add them up. this total is 1106, and since n11, the covariance of x and y is 110610110.6. the fact that the covariance is negative indicates that as age goes up strength tends to go down and vice versa. the formula for the sample correlation is corx, y covx, y sxsy 4.4. multivariate nongraphical eda 93 where sx is the standard deviation of x and sy is the standard deviation of y . in this example, sx 18.96, sy 6.39, so r 110.6 18.966.39 0.913. this is a strong negative correlation. subject id age strength gw 38 20 ja 62 15 tj 22 30 jma 38 21 jmo 45 18 jqa 69 12 aj 75 14 mvb 38 28 whh 80 9 jt 32 22 jkp 51 20 table 4.3 covariance sample data 4.4.5 covariance and correlation matrices when we have many quantitative variables the most common nongraphical eda technique is to calculate all of the pairwise covariances andor correlations and assemble them into a matrix. note that the covariance of x with x is the variance of x and the correlation of x with x is 1.0. for example the covariance matrix of table 4.5 tells us that the variances of x, y , and z are 5, 7, and 4 respectively, the covariance of x and y is 1.77, the covariance of x and z is 2.24, and the covariance of y and z is 3.17. similarly the correlation matrix in gure 4.6 tells us that the correlation of x and y is 0.3, the correlation of x and z is 0.5. and the correlation of y and z is 0.6. 94 chapter 4. exploratory data analysis subject id age strength age50 str19 product gw 38 20 12 1 12 ja 62 15 12 4 48 tj 22 30 28 11 308 jma 38 21 12 2 24 jmo 45 18 5 1 5 jqa 69 12 19 7 133 aj 75 14 25 5 125 mvb 38 28 12 9 108 whh 80 9 30 10 300 jt 32 22 18 3 54 jkp 51 20 1 1 1 total 0 0 1106 table 4.4 covariance calculation x y z x 5.00 1.77 2.24 y 1.77 7.0 3.17 z 2.24 3.17 4.0 table 4.5 a covariance matrix the correlation between two random variables is a number that runs from 1 through 0 to 1 and indicates a strong inverse relationship, no relationship, and a strong direct relationship, respectively. 4.5 multivariate graphical eda there are few useful techniques for graphical eda of two categorical random variables. the only one used commonly is a grouped barplot with each group representing one level of one of the variables and each bar within a group representing the levels of the other variable. 4.5. multivariate graphical eda 95 x y z x 1.0 0.3 0.5 y 0.3 1.0 0.6 z 0.5 0.6 1.0 table 4.6 a correlation matrix 4.5.1 univariate graphs by category when we have one categorical usually explanatory and one quantitative usually outcome variable, graphical eda usually takes the form of conditioning on the categorical random variable. this simply indicates that we focus on all of the subjects with a particular level of the categorical random variable, then make plots of the quantitative variable for those subjects. we repeat this for each level of the categorical variable, then compare the plots. the most commonly used of these are sidebyside boxplots, as in gure 4.15. here we see the data from eda3.dat, which consists of strength data for each of three age groups. you can see the downward trend in the median as the ages increase. the spreads iqrs are similar for the three groups. and all three groups are roughly symmetrical with one high strength outlier in the youngest age group. sidebyside boxplots are the best graphical eda technique for examining the relationship between a categorical variable and a quantitative variable, as well as the distribution of the quantitative variable at each level of the categorical variable. 4.5.2 scatterplots for two quantitative variables, the basic graphical eda technique is the scatterplot which has one variable on the xaxis, one on the yaxis and a point for each case in your dataset. if one variable is explanatory and the other is outcome, it is a very, very strong convention to put the outcome on the y vertical axis. one or two additional categorical variables can be accommodated on the scatterplot by encoding the additional information in the symbol type andor color. 96 chapter 4. exploratory data analysis g 21,42 42,62 62,82 10 15 20 25 30 35 age group strength figure 4.15 sidebyside boxplot of eda3.dat. 4.5. multivariate graphical eda 97 g g g g g g g g g g g g g g g g g g g g g g g g 20 30 40 50 60 70 80 10 15 20 25 30 35 age strength g g fdem frep mdem mrep figure 4.16 scatterplot with two additional variables. an example is shown in gure 4.16. age vs. strength is shown, and dierent colors and symbols are used to code political party and gender. in a nutshell you should always perform appropriate eda before further analysis of your data. perform whatever steps are necessary to become more familiar with your data, check for obvious mistakes, learn about variable distributions, and learn about relationships between variables. eda is not an exact science it is a very important art! 98 chapter 4. exploratory data analysis 4.6 a note on degrees of freedom degrees of freedom are numbers that characterize specic distributions in a family of distributions. often we nd that a certain family of distributions is needed in a some general situation, and then we need to calculate the degrees of freedom to know which specic distribution within the family is appropriate. the most common situation is when we have a particular statistic and want to know its sampling distribution. if the sampling distribution falls in the t family as when performing a ttest, or in the f family when performing an anova, or in several other families, we need to nd the number of degrees of freedom to gure out which particular member of the family actually represents the desired sampling distribution. one way to think about degrees of freedom for a statistic is that they represent the number of independent pieces of information that go into the calculation of the statistic, consider 5 numbers with a mean of 10. to calculate the variance of these numbers we need to sum the squared deviations from the mean. it really doesnt matter whether the mean is 10 or any other number as long as all ve deviations are the same, the variance will be the same. this make sense because variance is a pure measure of spread, not aected by central tendency. but by mathematically rearranging the denition of mean, it is not too hard to show that the sum of the deviations not squared is always zero. therefore, the rst four deviations can freely be any numbers, but then the last one is forced to be the number that makes the deviations add to zero, and we are not free to choose it. it is in this sense that ve numbers used for calculating a variance or standard deviation have only four degrees of freedom or independent useful pieces of information. in general, a variance or standard deviation calculated from n data values and one mean has n 1 df. another example is the pooled variance from k independent groups. if the sizes of the groups are n1 through nk, then each of the k individual variance estimates is based on deviations from a dierent mean, and each has one less degree of freedom than its sample size, e.g., ni 1 for group i. we also say that each numerator of a variance estimate, e.g., ssi, has ni1 df. the pooled estimate of variance is s2 pooled ss1 ssk df1 dfk and we say that both the numerator ss and the entire pooled variance has df1 4.6. a note on degrees of freedom 99 dfk degrees of freedom, which suggests how many independent pieces of information are available for the calculation. 100 chapter 4. exploratory data analysis'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = full_text_cleanup(unclean_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "def extract_text_langchain(pdf_path):\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    return \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "\n",
    "def lang_clean_text(text):\n",
    "    # text = text.replace(\"\\n\", \" \").strip()  \n",
    "    text = full_text_cleanup(text)\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "def split_text_into_sentences(text):\n",
    "    text_splitter = NLTKTextSplitter()\n",
    "    sentences = text_splitter.split_text(text)\n",
    "    cleaned_sentences = [sentence.replace(\"\\n\", \" \") for sentence in sentences]\n",
    "    return cleaned_sentences\n",
    "\n",
    "\n",
    "def create_resources(file_path):\n",
    "    lang_text = extract_text_langchain(eda_chatper)\n",
    "    lang_cleaned_text = lang_clean_text(lang_text)\n",
    "    lang_sentences = split_text_into_sentences(lang_cleaned_text[0])\n",
    "    return lang_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "sentence_chunks = create_resources(eda_chatper)\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")\n",
    "collection.add(\n",
    "    documents=sentence_chunks,\n",
    "    ids=[f\"{i}\" for i in range(len(sentence_chunks))]\n",
    ")\n",
    "\n",
    "# chroma_client.delete_collection(\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['for nonsymmetric distributions, the mean is the balance point if the histogram is cut out of some homogeneous stimaterial such as cardboard, it will balance on a fulcrum placed at the mean.  for many descriptive quantities, there are both a sample and a population version.  for a xed nite population or for a theoretic innite population described by a pmf or pdf, there is a single population mean which is a xed, often unknown, value called the mean parameter see section 3.5. on the other hand, the sample mean will vary from sample to sample as dierent samples are taken, and so is a random variable.  the probability distribution of the sample mean is referred to as its sampling distribution.  this term expresses the idea that any experiment could at least theoretically, given enough resources be repeated many times and various statistics such as the sample mean can be calculated each time.  often we can use probability theory to work out the exact distribution of the sample statistic, at least under certain assumptions.  the median is another measure of central tendency.  the sample median is 68 chapter 4. exploratory data analysis the middle value after all of the values are put in an ordered list.  if there are an even number of values, take the average of the two middle values.  if there are ties at the middle, some special adjustments are made by the statistical software we will use.  in unusual situations for discrete random variables, there may not be a unique median.  for symmetric distributions, the mean and the median coincide.  for unimodal skewed asymmetric distributions, the mean is farther in the direction of the pulled out tail of the distribution than the median is.  therefore, for many cases of skewed distributions, the median is preferred as a measure of central tendency.  for example, according to the us census bureau 2004 economic survey, the median income of us families, which represents the income above and below which half of families fall, was 43,318. this seems a better measure of central tendency than the mean of 60,828, which indicates how much each family would have if we all shared equally.  and the dierence between these two numbers is quite substantial.  nevertheless, both numbers are correct, as long as you understand their meanings.  the median has a very special property called robustness.  a sample statistic is robust if moving some data tends not to change the value of the statistic.  the median is highly robust, because you can move nearly all of the upper half andor lower half of the data values any distance away from the median without changing the median.  more practically, a few very high values or very low values usually have no eect on the median.  a rarely used measure of central tendency is the mode, which is the most likely or frequently occurring value.  more commonly we simply use the term mode when describing whether a distribution has a single peak unimodal or two or more peaks bimodal or multimodal.  in symmetric, unimodal distributions, the mode equals both the mean and the median.  in unimodal, skewed distributions the mode is on the other side of the median from the mean.  in multimodal distributions there is either no unique highest mode, or the highest mode may well be unrepresentative of the central tendency.  the most common measure of central tendency is the mean.  for skewed distribution or when there is concern about outliers, the median may be preferred.  4.2. univariate nongraphical eda 69 4.2.4 spread several statistics are commonly used as a measure of the spread of a distribution, including variance, standard deviation, and interquartile range.  spread is an indicator of how far away from the center we are still likely to nd data values.  the variance is a standard measure of spread.  it is calculated for a list of numbers, e.g., the n observations of a particular measurement labeled x1 through xn, based on the n sample deviations or just deviations.', 'it is called the quantilenormal or qn plot or more generality the quantilequantile or qq plot.  it is used to see how well a particular sample follows a particular theoretical distribution.  although it can be used for any theoretical distribution, we will limit our attention to seeing how well a sample of data of size n matches a gaussian distribution with mean and variance equal to the sample mean and variance.  by examining the quantilenormal plot we can detect left or right skew, positive or negative kurtosis, and bimodality.  the example shown in gure 4.8 shows 20 data points that are approximately normally distributed.  do not confuse a quantilenormal plot with a simple scatter plot of two variables.  the title and axis labels are strong indicators that this is a quantilenormal plot.  for many computer programs, the word quantile is also in the axis labels.  many statistical tests have the assumption that the outcome for any xed set of values of the explanatory variables is approximately normally distributed, and that is why qn plots are useful if the assumption is grossly violated, the pvalue and condence intervals of those tests are wrong.  as we will see in the anova and regression chapters, the most important situation where we use a qn plot is not for eda, but for examining something called residuals see section 9.4. for 84 chapter 4. exploratory data analysis basic interpretation of the qn plot you just need to be able to distinguish the two situations of ok points fall randomly around the line versus nonnormality points follow a strong curved pattern rather than following the line.  if you are still curious, here is a description of how the qn plot is created.  understanding this will help to understand the interpretation, but is not required in this course.  note that some programs swap the x and y axes from the way described here, but the interpretation is similar for all versions of qn plots.  consider the 20 values observed in this study.  they happen to have an observed mean of 1.37 and a standard deviation of 1.36. ideally, 20 random values drawn from a distribution that has a true mean of 1.37 and sd of 1.36 have a perfect bellshaped distribution and will be spaced so that there is equal area probability in the area around each value in the bell curve.  in gure 4.9 the dotted lines divide the bell curve up into 20 equally probable zones, and the 20 points are at the probability midpoints of each zone.  these 20 points, which are more tightly packed near the middle than in the ends, are used as the expected normal values in the qn plot of our actual data.  in summary, the sorted actual data values are plotted against expected normal values, and some kind of diagonal line is added to help direct the eye towards a perfect straight line on the quantilenormal plot that represents a perfect bell shape for the observed data.  the interpretation of the qn plot is given here.  if the axes are reversed in the computer package you are using, you will need to correspondingly change your interpretation.  if all of the points fall on or nearly on the diagonal line with a random pattern, this tells us that a histogram of the variable will show a bell shaped normal or gaussian distribution.  figure 4.10 shows all of the points basically on the reference line, but there are several vertical bands of points.  because the xaxis is observed values, these bands indicate ties, i.e., multiple points with the same values.  and all of the observed values are at whole numbers.  so either the data are rounded or we are looking at a discrete quantitative counting variable.  either way, the data appear 4.3. univariate graphical eda 85 2 0 2 4 0.00 0.05 0.10 0.15 0.20 0.25 0.30 expected normal value density g g g g g g gggggggg g g g g g g figure 4.9 a way to think about qn plots.  86 chapter 4. exploratory data analysis figure 4.10 quantilenormal plot with ties.  to be nearly normally distributed.', '4.3.1 histograms the only one of these techniques that makes sense for categorical data is the histogram basically just a barplot of the tabulation of the data.  a pie chart 4.3. univariate graphical eda 73 is equivalent, but not often used.  the concepts of central tendency, spread and skew have no meaning for nominal categorical data.  for ordinal categorical data, it sometimes makes sense to treat the data as quantitative for eda purposes you need to use your judgment here.  the most basic graph is the histogram, which is a barplot in which each bar represents the frequency count or proportion counttotal count of cases for a range of values.  typically the bars run vertically with the count or proportion axis running vertically.  to manually construct a histogram, dene the range of data for each bar called a bin, count how many cases fall in each bin, and draw the bars high enough to indicate the count.  for the simple data set found in eda1.dat the histogram is shown in gure 4.2. besides getting the general impression of the shape of the distribution, you can read ofacts like there are two cases with data values between 1 and 2 and there are 9 cases with data values between 2 and 3. generally values that fall exactly on the boundary between two bins are put in the lower bin, but this rule is not always followed.  generally you will choose between about 5 and 30 bins, depending on the amount of data and the shape of the distribution.  of course you need to see the histogram to know the shape of the distribution, so this may be an iterative process.  it is often worthwhile to try a few dierent bin sizesnumbers because, especially with small samples, there may sometimes be a dierent shape to the histogram when the bin size changes.  but usually the dierence is small.  figure 4.3 shows three histograms of the same sample from a bimodal population using three dierent bin widths 5, 2 and 1. if you want to try on your own, the data are in eda2.dat.  the top panel appears to show a unimodal distribution.  the middle panel correctly shows the bimodality.  the bottom panel incorrectly suggests many modes.  there is some art to choosing bin widths, and although often the automatic choices of a program like spss are pretty good, they are certainly not always adequate.  it is very instructive to look at multiple samples from the same population to get a feel for the variation that will be found in histograms.  figure 4.4 shows histograms from multiple samples of size 50 from the same population as gure 4.3, while 4.5 shows samples of size 100. notice that the variability is quite high, especially for the smaller sample size, and that an incorrect impression particularly of unimodality is quite possible, just by the bad luck of taking a particular sample.  74 chapter 4. exploratory data analysis x frequency 0 2 4 6 8 10 0 2 4 6 8 10 figure 4.2 histogram of eda1.dat.  4.3. univariate graphical eda 75 x frequency 5 0 5 10 15 20 25 0 5 15 25 x frequency 5 0 5 10 15 20 25 0 5 10 15 x frequency 5 0 5 10 15 20 25 0 2 4 6 8 figure 4.3 histograms of eda2.dat with dierent bin widths.  76 chapter 4. exploratory data analysis x frequency 5 0 5 10 20 0 2 4 6 8 x frequency 5 0 5 10 20 0 2 4 6 8 10 x frequency 5 0 5 10 20 0 2 4 6 8 x frequency 5 0 5 10 20 0 2 4 6 8 10 x frequency 5 0 5 10 20 0 2 4 6 8 12 x frequency 5 0 5 10 20 0 2 4 6 8 x frequency 5 0 5 10 20 0 2 4 6 8 10 x frequency 5 0 5 10 20 0 2 4 6 8 10 x frequency 5 0 5 10 20 0 2 4 6 8 10 figure 4.4 histograms of multiple samples of size 50.  4.3. univariate graphical eda 77 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 20 x frequency 5 0 5 10 20 0 5 10 15 20 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 x frequency 5 0 5 10 20 0 5 10 15 figure 4.5 histograms of multiple samples of size 100.', '86 chapter 4. exploratory data analysis figure 4.10 quantilenormal plot with ties.  to be nearly normally distributed.  in gure 4.11 note that we have many points in a row that are on the same side of the line rather than just bouncing around to either side, and that suggests that there is a real nonrandom deviation from normality.  the best way to think about these qn plots is to look at the low and high ranges of the expected normal values.  in each area, see how the observed values deviate from what is expected, i.e., in which x observed value direction the points appear to have moved relative to the perfect normal line.  here we observe values that are too high in both the low and high ranges.  so compared to a perfect bell shape, this distribution is pulled asymmetrically towards higher values, which indicates positive skew.  also note that if you just shift a distribution to the right without disturbing its symmetry rather than skewing it, it will maintain its perfect bell shape, and the points remain on the diagonal reference line of the quantilenormal curve.  of course, we can also have a distribution that is skewed to the left, in which case the high and low range points are shifted in the observed value direction towards lower than expected values.  in gure 4.12 the high end points are shifted too high and the low end points are shifted too low.  these data show a positive kurtosis fat tails.  the opposite pattern is a negative kurtosis in which the tails are too thin to be bell shaped.  4.3. univariate graphical eda 87 figure 4.11 quantilenormal plot showing right skew.  figure 4.12 quantilenormal plot showing fat tails.  88 chapter 4. exploratory data analysis figure 4.13 quantilenormal plot showing a high outlier.  in gure 4.13 there is a single point that is othe reference line, i.e.  shifted to the right of where it should be.  remember that the pattern of locations on the expected normal value axis is xed for any sample size, and only the position on the observed axis varies depending on the observed data.  this pattern shows nearly gaussian data with one high outlier.  finally, gure 4.14 looks a bit similar to the skew left pattern, but the most extreme points tend to return to the reference line.  this pattern is seen in bimodal data, e.g.  this is what we would see if we would mix strength measurements from controls and muscular dystrophy patients.  quantilenormal plots allow detection of nonnormality and diagnosis of skewness and kurtosis.  4.4 multivariate nongraphical eda multivariate nongraphical eda techniques generally show the relationship between two or more variables in the form of either crosstabulation or statistics.  4.4. multivariate nongraphical eda 89 figure 4.14 quantilenormal plot showing bimodality.  4.4.1 crosstabulation for categorical data and quantitative data with only a few dierent values an extension of tabulation called crosstabulation is very useful.  for two variables, crosstabulation is performed by making a twoway table with column headings that match the levels of one variable and row headings that match the levels of the other variable, then lling in the counts of all subjects that share a pair of levels.  the two variables might be both explanatory, both outcome, or one of each.  depending on the goals, row percentages which add to 100 for each row, column percentages which add to 100 for each column andor cell percentages which add to 100 over all cells are also useful.  here is an example of a crosstabulation.  consider the data in table 4.1. for each subject we observe sex and age as categorical variables.  table 4.2 shows the crosstabulation.']]\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"what is Central tendency ?\"],\n",
    "    n_results=4\n",
    ")\n",
    "\n",
    "print(results[\"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Intergration \n",
    "\n",
    "We are using Llama3.2 with knowledge cut off (December 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def ask_ollama(query, context=None):\n",
    "\n",
    "    context = \"-\".join(context) if context else \"No context provided.\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "            You are an advanced Retrieval-Augmented Generation (RAG) system designed to provide highly accurate and contextually relevant responses. Use *only* the information provided in the context below to generate your answer. Do not use any prior knowledge or external sources. If the context does not contain enough information to answer the question, explicitly state: \"I cannot answer this question based on the provided information.\"\n",
    "\n",
    "            ## Instructions:\n",
    "            - Analyze the retrieved context carefully to extract the most relevant details.\n",
    "            - Ensure that your answer is comprehensive, well-structured, and directly addresses the user's question.\n",
    "            - If multiple pieces of evidence exist in the context, synthesize them for a cohesive response.\n",
    "            - If the context is unclear, ambiguous, or conflicting, acknowledge this uncertainty in your response.\n",
    "            - Do not assume or infer facts beyond what is stated in the provided context.\n",
    "\n",
    "            ## Context:\n",
    "            {context}\n",
    "\n",
    "            ## Question:\n",
    "            {query}\n",
    "\n",
    "            ## Answer:\n",
    "            \"\"\"\n",
    "    \n",
    "    grok_prompt = f\"\"\"\n",
    "        ### Prompt for RAG System\n",
    "\n",
    "            **Instruction:**\n",
    "            You are an AI designed to answer queries using a two-step process involving context retrieval and knowledge-based answering. Here's how you should proceed:\n",
    "\n",
    "            1. **Context Retrieval (Step 1):**\n",
    "            - **Context:** {context}\n",
    "            - **Query:** {query}\n",
    "\n",
    "            First, attempt to answer the query using the provided context. Look for relevant information within the context that directly relates to the query. If you can answer the query comprehensively using only this context, do so. If you cannot:\n",
    "\n",
    "            2. **Knowledge-Based Answer (Step 2):**\n",
    "            - If the context does not provide enough information to answer the query accurately, or if the query is not adequately addressed by the context, use your pre-existing knowledge to answer the query. \n",
    "            - Be clear that you are now using your knowledge by starting your response with \"Based on my knowledge:\".\n",
    "\n",
    "            **Guidelines:**\n",
    "            - **Accuracy:** Prioritize accuracy. If the context does not provide a clear answer and your knowledge is uncertain or outdated, acknowledge this by saying, \"I'm not certain about this, but based on my knowledge:\".\n",
    "            - **Completeness:** If part of the query can be answered with context but not fully, use context for what you can and supplement with knowledge.\n",
    "            - **Citations:** When answering from context, if possible, reference or quote directly from the context by using quotation marks or by specifying where in the context the answer was found (e.g., \"According to the context...\").\n",
    "            - **Admit Limitations:** If neither the context nor your knowledge can provide an answer, admit this by saying, \"I do not have enough information to answer this query adequately.\"\n",
    "\n",
    "            **Example Response Formats:**\n",
    "\n",
    "            - **From Context:** \"The context states that the boiling point of water at sea level is 100°C.\"\n",
    "            - **From Knowledge:** \"Based on my knowledge, the average adult human body contains approximately 60% water.\"\n",
    "            - **Mixed:** \"From the context, we learn that the Eiffel Tower was completed in 1889. Based on my knowledge, it was designed by Gustave Eiffel.\"\n",
    "            - **Admitting Limitation:** \"I do not have enough information to answer this query adequately.\"\n",
    "\n",
    "            **Proceed:**\n",
    "            Now, attempt to answer the query provided:\n",
    "\n",
    "            **Query:** {query}\n",
    "\n",
    "            Your answer should be just explain of your understanding of the question. Dont list steps or any other things. Just explain the concept. Dont say Based on my knowledge or Based on the provided context, on your answer\n",
    "            Just answer the question directly and in detail simple way\n",
    "            DONT MENTION ABOUT THE CONTEXT OR THE QUESTION IN YOUR ANSWER\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"llama3.2\",  \n",
    "        \"prompt\": grok_prompt,\n",
    "        \"stream\": False \n",
    "    }\n",
    "\n",
    "    response = requests.post(OLLAMA_URL, json=payload)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(data[\"response\"])\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "qn1 = \"What is Central tendency ?\"\n",
    "qn2 = \"What Non-graphical methods of data presentation involves ?\"\n",
    "qn3 = \"Is (IQR) is a robust measure of spread ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag(question):\n",
    "    response_query = collection.query(query_texts=[question], n_results=4)\n",
    "    context = response_query[\"documents\"][0]\n",
    "    response = ask_ollama(question, context)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central tendency is a statistical measure that describes the central or typical value of a dataset. It provides an overview of the data's main characteristics, allowing for easy interpretation and comparison. The most commonly used measures of central tendency are the mean, median, and mode.\n",
      "\n",
      "The mean is calculated by summing up all the values in the dataset and dividing by the number of values. It is sensitive to extreme values and can be affected by outliers.\n",
      "\n",
      "The median is the middle value in a sorted dataset. If there is an even number of values, the median is the average of the two middle values. The median is more resistant to outliers than the mean.\n",
      "\n",
      "The mode is the most frequently occurring value in the dataset. A dataset can have one mode (unimodal), two modes (bimodal), or no mode at all ( multimodal).\n",
      "\n",
      "Central tendency is important because it helps us understand the main characteristics of a dataset, which can inform decisions and predictions. It is widely used in statistics, data analysis, and various fields such as business, economics, medicine, and social sciences.\n"
     ]
    }
   ],
   "source": [
    "ask_rag(qn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-graphical methods of data presentation involve statistical methods such as calculating means, variances, standard deviations, t-tests, ANOVA, regression analysis, hypothesis testing, confidence intervals, and non-parametric tests. These methods use mathematical formulas and equations to summarize and analyze data without the need for visual representations like charts or graphs.\n"
     ]
    }
   ],
   "source": [
    "ask_rag(qn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IQR (Interquartile Range) is a robust measure of spread because it is not significantly affected by extreme values or outliers. Half of the data points fall within an interval whose width equals the IQR, meaning that even if most values are concentrated in the middle part of the distribution, the IQR remains relatively stable and resistant to changes caused by outliers. This property makes the IQR a more robust measure of spread compared to variance or standard deviation.\n"
     ]
    }
   ],
   "source": [
    "ask_rag(qn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
